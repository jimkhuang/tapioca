作者xa9277178 (楓曦)看板Stock標題Re: [新聞] ChatGPT的「幻覺」根本無法解時間Thu Aug 10 10:25:36 2023
※ 引述《breeze0817 ()》之銘言：
: 原文標題：
: 技術專家：ChatGPT的「幻覺」根本無法解決
: 原文連結：
: https://reurl.cc/N0KRy6
: 發布時間：
: 2023/08/09 15：58
: 記者署名：
: 鉅亨網新聞中心
: 原文內容：
: 掀起 AI 浪潮的 ChatGPT 儘管可以協助解決很多事情，但也因為會產生虛假的訊息，或
: 誤導性的內容，產生「人工智慧幻覺」(hallucinations) 而飽受批評，專家認為這種「
: 幻覺」受到技術與需求之間的限制，根本無法被解決。
: 財富網中文網報導，與 ChatGPT 或其他 AI 聊天機器人相處足夠長的時間，很快就會發
: 現它們有時會「編造答案」，這種現象被稱為「幻覺」，即機器人做出的捏造或純粹的編
: 造，對於那些試圖讓 AI 系統編寫文檔並完成工作的企業、組織甚至學生，這是一個致命
: 的問題。
: 聊天機器人 Claude 2 的製造商 Anthropic 共同創始人 Daniela Amodei 認為，現在沒
: 有任何一種模型能夠完全避免產生幻覺，因為它們的設計初衷只是用來預測下一個單詞，
: 因此模型肯定會有一定的錯誤率。
: 包括 ChatGPT 的製造商 OpenAI、Anthropic 及其他大型語言模型的 AI 系統主要開發者
: 都表示，他們正努力讓這些系統變得更準確，但目前尚不清楚需要多長時間才能達到足夠
: 的可靠性。
: 不過，華盛頓大學計算語言學實驗室的主任、語言學教授 Emily Bender 認為，「這個問
: 題無法完全解決，因為技術與實際需求之間存在固有的不匹配。」
: Google 已向新聞機構推銷一款新聞寫作 AI 產品，而作為與 OpenAI 合作的一部分，美
: 聯社也在探索使用這項技術，而 OpenAI 正在付費使用美聯社的部分存檔文本來改進其 A
: I 係統。因此，生成式 AI 的可靠性至關重要，麥肯錫全球研究院（McKinsey）預計，這
: 將為全球經濟帶來相當於 2.6 兆至 4.4 兆美元的收入。
: 心得/評論：
: chatGPT訪問量一再下滑
: 一窩蜂投資ai伺服器去訓練語言模型的
: 越來越有元宇宙的fu糗了
: 散戶：這次不一樣！！


純論AI的話 這個問題是一定可以被解決的

討論chatGPT肯定是無解 頂多提升準確度 提升到你認知不到「錯覺」

我自己是覺得用錯覺這個詞來闡述這個問題 好像滿奇怪的

簡單思考一下就知道了

假設AI是個人 他肯定會有他自己的認知

他的認知來自於他的資料庫

但誰能保證資料庫裡面的資料100%是對的？

畢竟資料庫的資料也是人為產出，或是他的開發者灌入的

最終都會限縮於：「什麼是對的」的這個命題

並且有一個很弔詭的事情是這樣的

「你憑什麼認為AI給出的答案是錯的，便稱作幻覺，難道就是基於人類的共同認知嗎？」

這個東西討論到後面就變成哲學問題了，所以不贅述。

但核心概念是這樣

一個人，老師，教授，都會有認知錯誤，都會出錯，為什麼人工智能不會

基於什麼樣的原因，你覺得他能掌握100%的真理，這樣想想不就知道了

但是基於網速科技提升，資料處理能力加強，程序優化肯定能再進步

回歸前面所說的，只要把AI的範圍侷限性拉大，就能提升正確率阿

比如你設定這個AI就叫做金融投資機器人，甚至叫做台灣金融投資機器人

並且串聯所有台灣銀行、投資平台、各大當鋪的內部資料

那肯定是強的沒話說。


你期待一個AI機器人，能貫通世界上所有領域的事情，給你都是正確的答案

那肯定是不切實際。


更何況一定一堆人是這樣：「請給我一個完美的python程序。」

「幹，你這個根本不完美」

你自己都定義不出來，都不知道什麼叫做完美了，AI怎麼可能會知道

你怎麼連話都說不清楚~


我自己是覺得AI很神啦，沒有戰文組的意思，但對於邏輯思考，表述能力不好的人來說

可能不好用

我在某平台上面，買了好幾次程式課，每次都沒有耐心看完，把作業做完。

chatgpt用了兩三天，已經弄出了兩個簡單的小程式，改善我工作上面的需求

平心而論，我就算認真把課程都上完，自己寫，寫個1個月，大概都弄不出來。

AI最強大的，就是他給你的資訊裡面，是有一定程度的正確率的，

你本來就要能分辨，什麼東西對你有用，然後再引導他幫助你，給出更多有用的資訊，

最後幫助你完成你的目標。

推 y2468101216 : 他這個幻覺是幻覺一些很明顯的是 08/10 10:26

→ y2468101216 : 比如日本二戰被兩顆原子彈轟炸，他會有機率說錯 08/10 10:27

→ y2468101216 : 回歸到程式面的話，簡單的程式也會幻覺。 08/10 10:28

→ y2468101216 : 比如一個程式函數只接受一個參數，他會寫兩個 08/10 10:29

推 Isopod      : 你是不是不知道它幻覺產生的內容是什麼？有一些根本 08/10 10:29

→ Isopod      : 錯誤的離譜。而且當你跟他要參考資料時，好一點會跟 08/10 10:29

→ Isopod      : 你說它理解錯了。可怕一點的會生成不存在的參考資料 08/10 10:29

→ Isopod      : 出來 08/10 10:29

→ Isopod      : 他根本不是只參考資料庫給答案，是給出根本不在資料 08/10 10:30

→ Isopod      : 庫中自己幻想的答案 08/10 10:30

推 strlen      : 生成不存在的東西 這個人類最拿手的勒 打開電視隨便 08/10 10:32

推 shawkwei    : 4909新復興營收月增跟年增都100% 08/10 10:32

→ strlen      : 轉一間新聞台都是生成不存在的東西R XD 08/10 10:33

推 Isopod      : 我覺得你要先把你的程式課上好，再來討論耶QQ 08/10 10:34

→ strlen      : 股板一堆財經新聞 你能分辨哪些是真哪些是假嗎 嘻嘻 08/10 10:34

推 kkithh      : 敎授那等級的人會知道他講的話是不是準確，就算表 08/10 10:35

→ kkithh      : 面堅持是準確但內心會有底，但AI目前根本無法判斷 08/10 10:35

→ kkithh      : 自己講的內容是否準確 08/10 10:35

→ strlen      : 齁齁 所以你就能判斷你眼前看到每件資訊都為真嗎XD 08/10 10:35

→ kkithh      : 所以你會看到它一本正經在講幹話 08/10 10:36

→ IBIZA       : 會說是幻覺, 就是因為這些不是資料生成 08/10 10:36

→ kkithh      : 我會知道我不確定阿，知道差別嗎？ 08/10 10:36

→ strlen      : 每天晚上隨便打開一台政論節目 不也都在講幹話 08/10 10:36

→ strlen      : 收視率還高到要命 大家愛看得要死 XDDDDD 08/10 10:37

→ IBIZA       : 比方說 他叫他寫程式, 他會丟給一個不存在的套件 08/10 10:37

→ IBIZA       : 一本正經的用這個套件開發程式碼 08/10 10:37

→ IBIZA       : 這已經不是錯不錯的問題 08/10 10:37

→ strlen      : 你跟不同信仰跟政治的人說話 感覺根本想平行宇宙 08/10 10:38

→ strlen      : 怎麼 判斷你的說法是正確 他的說法是錯誤？ 08/10 10:38

→ IBIZA       : 你先了解一下為什麼會用「幻覺」來形容 08/10 10:38

→ z7956234    : 大型網頁版siri，一樣撈資料回答一樣會出錯，都一 08/10 10:39

→ z7956234    : 樣。 08/10 10:39

→ strlen      : 要捏造一個不存在的東西 這個人類更拿手 而且生活中 08/10 10:39

→ IBIZA       : 人類也會有類似的錯覺, 例如曼德拉效應之類的 08/10 10:39

噓 Isopod      : 設定的範圍怎麼會是無限，就是餵給他的資料而已。雖 08/10 10:39

→ Isopod      : 然龐大但不是無限。 08/10 10:39

→ strlen      : 這種案例比你想像中的還要多 你用google搜也不一定 08/10 10:39

→ strlen      : 正確 一堆內容農場東拼西湊 wiki也是亂寫一通 08/10 10:39

推 yuzulee     : 推文都滿溫和的耶... 08/10 10:39

→ Isopod      : 文章嘴文組，結果自己對於AI的理解也是幻想 08/10 10:39

→ twistfist   : 老師教授可以直接說這我也不確定，ai 會不懂裝懂 08/10 10:40

→ a12838910   : 超導呼... 08/10 10:40

→ strlen      : 正確性這種東西 人類自己都沒辦法保證了 還要求才剛 08/10 10:40

→ strlen      : 有點小發展的AI 08/10 10:40

→ IBIZA       : 幻覺跟正確性不完全是同一個議題 08/10 10:40

→ Isopod      : 那個str也在亂扯，就是知道瞎掰不對，所以才希望AI 08/10 10:40

→ Isopod      : 不要瞎掰。 08/10 10:41

→ IBIZA       : 政論節目的幹話 大多數還是在有所本的基礎上扭曲 08/10 10:41

→ strlen      : 問題在於 你怎麼定義瞎掰？ 08/10 10:41

→ IBIZA       : 事實來的  AI的幻覺就不知道哪來的 08/10 10:41

推 ab4daa      : 無罪 08/10 10:41

→ Isopod      : 他可以根據資料庫理解錯誤，這是可以修正的。但是瞎 08/10 10:41

→ Isopod      : 掰不存在於資料庫的東西，這個就是現在的問題 08/10 10:41

→ strlen      : 你先定義瞎掰是什麼 我們再來談瞎掰對不對嘛 合理阿 08/10 10:41

→ strlen      : 我就說即使你用google 上面一堆東西也都瞎掰 08/10 10:42

→ IBIZA       : 我想我前面已經對於AI幻覺有很清楚地描述 08/10 10:42

推 stosto      : Ai 大部分都是最佳近似解….到底要怎麼不講幹話 08/10 10:42

→ strlen      : 但google搜尋就沒用了嗎？你打開電視上面一堆假新聞 08/10 10:42

→ IBIZA       : 瞎掰的範疇很大 有所本跟無所本 是兩回事 08/10 10:42

→ IBIZA       : 瞎掰的範疇很大 有所本跟無所本 是兩回事 08/10 10:42

→ strlen      : 那新聞就通通可以關一關了是嗎 08/10 10:42

→ Isopod      : 是呀 就是因為有不懂裝懂的人自己胡扯呀 08/10 10:43

→ stosto      : 一堆幹話連人類都講不出來 08/10 10:43

→ IBIZA       : 假新聞也是  在有所本的新聞上亂寫 跟 報導一則完全 08/10 10:43

→ IBIZA       : 虛構的新聞是兩回事 08/10 10:43

→ strlen      : AI的瞎掰當然也是有所本啊 它的模式跟人一模一樣 08/10 10:43

→ IBIZA       : 就跟你說 AI幻覺 是無所本 08/10 10:43

噓 aynmeow5566 : 問題是無中生有捏造資料 為了湊答案捏造論文出處 要 08/10 10:43

→ aynmeow5566 : 他驗算等號左邊明顯不等於右邊還能說是相等的 快點 08/10 10:43

→ aynmeow5566 : 解決啦 08/10 10:43

→ strlen      : 人要瞎掰 不就是東拼西湊 假新聞和政論名嘴也是 08/10 10:43

→ SiFox       : 瞎掰扯謊憑空幻覺，只要有0.001%的可能，AI就玩完 08/10 10:43

→ IBIZA       : 我前面舉的例子你看了嗎 08/10 10:43

→ IBIZA       : 我前面舉的例子你看了嗎 08/10 10:43

→ strlen      : 一本正經瞎掰 你去看看LBJ嘛 收視多好 資訊都正確嗎 08/10 10:44

→ stosto      : Ai核心都是資料集小辦法去生成一個f(x), 你就算帶 08/10 10:44

→ stosto      : 入一個從未出現的x大部分也都可以得到一個y 08/10 10:44

→ Isopod      : 沒有人說AI沒有用，只是在討論他瞎掰的問題有點嚴重 08/10 10:44

→ Isopod      : 。就像google出來的東西，很多也是垃圾，所以才要使 08/10 10:44

→ Isopod      : 用者才要自己找方法判讀。但AI目前沒有辦法找方法判 08/10 10:44

→ Isopod      : 讀自己輸出的資料是不是瞎掰的。這樣你懂嗎 08/10 10:44

→ stosto      : 這個y扯不扯是另外一回事，幻覺就是這個y 08/10 10:45

→ SiFox       : 政客扯謊帶來多少美好崇景，謊言搓破帶來多少痛苦 08/10 10:45

推 ckp4131025  : AI會自創api很正常啊，因為人類也會 08/10 10:45

→ ckp4131025  : 咁，這個api沒有嗎，我以為會有 08/10 10:45

→ SiFox       : 人類有政客就夠了，不需要AI來自尋煩惱甚至帶來毀滅 08/10 10:45

→ stosto      : 一堆人不懂演算法在那邊扯幻覺 08/10 10:45

→ IBIZA       : 對 人類也會無中生有  但目前無法理解為什麼AI會 08/10 10:45

→ IBIZA       : 而且無法解決 所以只好縮限AI回答能力 08/10 10:46

→ stosto      : 就不是無中生有，而是帶入一個x以他建造出來的模型 08/10 10:46

→ stosto      : 就是會丟出一個y給妳 08/10 10:46

推 deadair     : 我問他變形記他跟我講悲慘世界 整個不知道在幹嘛XD 08/10 10:47

→ IBIZA       : 曼德拉效應我不知道大家知不知道 08/10 10:47

推 Isopod      : 不在資料集裡的y就是我們在討論的幻覺啦，自己說從 08/10 10:47

→ Isopod      : 資料集生成，啊生成資料集以外的結果怎麼說？ 08/10 10:47

→ stosto      : 給你兩組x,y 讓你求一元一方程式，非資料集內的x帶 08/10 10:48

→ stosto      : 入後就是會幫你算出y 08/10 10:48

推 strlen      : 反駁不了我就惱羞喔 XD 08/10 10:48

→ strlen      : 事實是 我們的現實本來就被巨大的假資訊包圍 08/10 10:48

→ stosto      : 你會說這個y是幻覺？ 08/10 10:48

→ Isopod      : 已經說了，他可以回答錯誤，這只是演算法不夠強。但 08/10 10:49

→ Isopod      : 這跟產出不存在的資料是兩回事 08/10 10:49

→ strlen      : 然後我們的認知不想承認這件事實 08/10 10:49

→ IBIZA       : deadair那個例子還不算 08/10 10:49

→ strlen      : 不然你看看對岸就知道 你看看他們的現實和你的一不 08/10 10:49

→ strlen      : 一樣嘛 08/10 10:49

→ strlen      : 現實的假資訊跟AI給的一樣 半真半假 你敢說你分得出 08/10 10:50

→ IBIZA       : 我不知道是你一直要講現實要幹嘛 有人說人不會無中 08/10 10:50

→ IBIZA       : 生有嗎? 08/10 10:50

→ IBIZA       : 人當然是會 08/10 10:50

→ Isopod      : str真的來亂的，直接忽略就好 08/10 10:50

→ stosto      : Ai就只是個求函數的演算法、而目前很少演算法可以 08/10 10:50

→ stosto      : 求出現實環境的完美函數 08/10 10:50

→ IBIZA       : 你餵假資料給AI  AI當然也是會吃了假資料回答你假 08/10 10:50

→ IBIZA       : 資料的內容  問題就不是假資料啊 08/10 10:51

→ Isopod      : stosto至少還是在討論問題 08/10 10:51

→ stosto      : 所以求得的都是逼近函數 08/10 10:51

→ strlen      : 你根本反駁不了就說人家來亂喔？不然你解釋一下怎麼 08/10 10:51

推 SiFox       : 假資訊讓AI學習，AI再扯謊一下，是打算讓世界多亂？ 08/10 10:51

→ strlen      : 解決現實裡的假資訊 08/10 10:51

→ stosto      : 而逼近函數就是以資料集去逼出來的 08/10 10:51

→ IBIZA       : @stosto 其實要說一切都是函數結果當然是可以 08/10 10:51

→ SiFox       : 更別說，不用假資訊的狀況下AI就會扯謊了 08/10 10:51

→ strlen      : 然後你再來想想要怎麼解決AI給的假資訊 這才合理吧 08/10 10:52

→ stosto      : 你有假資料本來就會把一個不錯的函數帶偏 08/10 10:52

→ Isopod      : 現在是在討論現實世界假資訊？你根本沒弄懂命題就不 08/10 10:52

→ Isopod      : 要在那邊吵。 08/10 10:52

→ IBIZA       : 問題就是現在連專家都不知道為什麼會這樣 08/10 10:52

→ strlen      : AI是現實的資料進去生成的對吧 那何以保證現實資料 08/10 10:52

→ stosto      : 但在處理大資料也會去過濾假資料 08/10 10:52

→ strlen      : 的的正確性 08/10 10:52

→ IBIZA       : @stosto 你講的東西其實對於認識這件事情毫無幫助 08/10 10:52

→ tony15899   : wiki之前有個自己編出一段古俄羅斯史的 很像這種 08/10 10:53

→ IBIZA       : 專家要是知道就不會有這篇新聞了 好嗎 08/10 10:53

→ SiFox       : 對！最麻煩、最可怕的是"不知道為什麼" 08/10 10:53

→ Isopod      : 如果把假資料給AI，他當然就以這個假資料產出結果， 08/10 10:53

→ Isopod      : 但是這還是「有所本」。目前的問題是，他所產出的資 08/10 10:53

→ Isopod      : 料，並不在餵給他的資料裡 08/10 10:53

→ SiFox       : 光這點，AI基本無法發展 08/10 10:53

→ IBIZA       : 目前就是不知道為什麼會這樣 所以只好縮限AI回答能 08/10 10:53

→ IBIZA       : 力  避免AI幻覺 08/10 10:53

→ stosto      : 原文是說無法解決而非不知道問題點 08/10 10:54

→ strlen      : 現實的假資訊不會影響到AI嗎 你確定嗎 08/10 10:54

→ SiFox       : 專家就是因為不知道，才僅能用"幻覺"來形容狀況 08/10 10:54

推 herculus6502: 光要分辨哪裏有錯就搞死你，自動產生程式根本科科 08/10 10:54

→ Isopod      : 所以現在討論的不是，把假資料餵給AI產出的奇怪答案 08/10 10:54

→ Isopod      : 叫幻覺。而是不知道為什麼會產出根本不在資料集的東 08/10 10:54

→ IBIZA       : 你相信不相信都改不瞭現況 08/10 10:54

→ Isopod      : 西。 08/10 10:54

→ strlen      : 命題沒有問題啊 不然你確定餵給AI的東西100%正確嗎 08/10 10:54

→ stosto      : 所有做ai 的如果不知道會有這種無中生有的問題可以 08/10 10:55

→ SiFox       : 不知道問題點，請問該怎麼解決？ XD 08/10 10:55

→ stosto      : 回去重修了 08/10 10:55

→ Isopod      : 目前AI幻覺真的找不太到原因，AI演算法基本上是個黑 08/10 10:55

→ Isopod      : 盒子，中間運算過程很難追蹤 08/10 10:55

→ IBIZA       : 這邊講得不知道 指的是不知道為什麼發生  而不是不 08/10 10:55

→ a8521abcd   : 那根本不是資料庫問題，就是chatgpt 自己在瞎掰 08/10 10:55

→ stosto      : 幻覺這件事情就只是錯誤率而已 08/10 10:55

→ IBIZA       : 知道會發生這種情況 08/10 10:55

→ IBIZA       : 不是錯誤率而已 08/10 10:55

→ Isopod      : 我覺得sto講的是錯誤答案，不是我們在討論的幻覺 08/10 10:56

→ stosto      : 這東西就一定會發生 到底是多專家才覺得不會發生 08/10 10:56

→ SiFox       : 黑盒子形容的很貼切。這就是AI無法發展的原因 08/10 10:56

→ strlen      : 無中生有不就是人類思考的特性之一？ 08/10 10:56

→ IBIZA       : 到底多專家  這一篇就是大學系主任? 08/10 10:56

→ strlen      : AI是不是就是以模仿人類為目標？ 08/10 10:57

→ IBIZA       : ChatGPT的專家也說過類似的話 08/10 10:57

推 max2604     : 我問他 08/10 10:57

→ strlen      : 那就這一點 AI還真的達到目標了勒 XD 08/10 10:57

→ stosto      : 在文字拓撲學裡的網路大成這樣，你函數在某一段錯 08/10 10:57

→ stosto      : 誤後面會偏離的很嚴重 08/10 10:57

→ Isopod      : @strlen 就算是資料有問題，那也是根據資料產生的回 08/10 10:57

→ Isopod      : 答，雖然答案是錯誤的，但是就過程而言是正確的，這 08/10 10:57

→ Isopod      : 是資料的問題。但現在的問題是他沒有根據資料，自己 08/10 10:57

→ Isopod      : 創造了答案。這樣你清楚問題了嗎？ 08/10 10:57

→ max2604     : 桃樂比是誰，他給我回答其他東西XD 08/10 10:58

→ stosto      : 這明明大家都知道，現在扯不知道幻覺怎麼出現 08/10 10:58

→ SiFox       : 只要有黑盒子存在，機器人三法則必然無效。 08/10 10:58

→ max2604     : 然後問他六位數加法他兩次答案不一樣 08/10 10:58

噓 ji3g4zo6    : https://i.imgur.com/6C7jFzQ.jpg 這就是幻覺好嗎 08/10 10:58

→ ji3g4zo6    : 跟資料庫有什麼關系？ 08/10 10:58

→ SiFox       : 所以AI發展到盡頭，僅會是假裝有人性的一團混亂 08/10 10:59

→ Isopod      : 原子彈那個就是我們在討論的幻覺。當然一定是演算法 08/10 10:59

→ Isopod      : 本身有問題，但就是不知道為什麼會有這樣的問題 08/10 10:59

→ IBIZA       : 之前還有一個例子 08/10 11:00

→ IBIZA       : 有一個律師用ChatGPT產生出庭文件 08/10 11:00

→ IBIZA       : 律師看內容覺得沒問題就提交出去 08/10 11:00

→ IBIZA       : 但文件裡面用來論述主張的判例不存在 08/10 11:00

→ IBIZA       : ChatGPT幻想出了一個案件的詳細情況 當作判例 08/10 11:01

→ Isopod      : 從資料庫找可能答案，但沒找到就應該產出沒有。而不 08/10 11:01

→ Isopod      : 是自己編造一個回應。 08/10 11:01

→ ji3g4zo6    : 不懂幻覺的人 建議去看一下李宏毅教授的課 08/10 11:01

推 tony15899   : https://reurl.cc/zYnryy 這個 08/10 11:01

推 ckp4131025  : 生成式AI並不是基於資料查詢的AI, 所以不設限制的 08/10 11:02

→ ckp4131025  : 話甚至連算數都不會 08/10 11:02

推 kqalea      : 抱歉～ Yann LeCun 早就講過～只靠語言沒法完全理解 08/10 11:02

→ IBIZA       : 我前面講的那個套件的例子, 是有人要chatGPT用pytho 08/10 11:03

→ IBIZA       : n開發一個程式, chatGPT就給了他一個基於OOOO套件的 08/10 11:04

→ IBIZA       : 程式, 內容看起來都沒問題, 但要去找oooo套件來安裝 08/10 11:04

→ IBIZA       : 就不存在這個套件 08/10 11:04

→ stkoso      : 這個缺點大家都知道 但只要利大於弊就會有人用 08/10 11:05

→ IBIZA       : 如果你不太能理解這個問題, 這就好像你要chatGPT用 08/10 11:05

→ stkoso      : 從神經網路剛開始發展的時候就知道的事情 08/10 11:05

→ IBIZA       : 外語寫一封信, 結果chatGPT告訴你它用阿里不達國語 08/10 11:06

→ IBIZA       : 寫, 而且還真的寫出來了 08/10 11:06

→ IBIZA       : 但不存在這國語言, 自然內容你也無從理解 08/10 11:07

→ leo125160909: 公三小 08/10 11:07

噓 ji3g4zo6    : 阿假設資料為真 後面的答案不就是幻覺了 還是連歷 08/10 11:07

→ ji3g4zo6    : 史這種事情都可以發揮想像力來回答？ 08/10 11:07

→ IBIZA       : 這個跟之前wiki上的假俄羅斯歷史有點類似 08/10 11:07

推 stosto      : 你用兩組資料求出y=ax+b 然後說不在這兩組x就不能 08/10 11:08

→ stosto      : 求出y, 天才 08/10 11:08

推 GAIKING     : 這命題不就跟美國某個律師用ai抓判例，結果ai提供 08/10 11:09

→ GAIKING     : 的完全就是自己生成而不是真的有這個東西，然後律師 08/10 11:09

→ GAIKING     : 不知道還拿去用出大事 08/10 11:09

推 Isopod      : chatgpt就是不是算數學AI，他不是只是一個簡單函式 08/10 11:10

→ Isopod      : 好嗎 08/10 11:10

推 stosto      : 所有的ai都在求function….. 08/10 11:10

→ stosto      : 就說在求逼近函數 08/10 11:11

→ stkoso      : 當年愛迪生為了打擊交流電整天喊會電死人 08/10 11:11

→ stosto      : 要求函數的話資料集一定有限 08/10 11:11

→ stkoso      : 直到今天都還有人會被電死 但是呢? 08/10 11:11

→ stosto      : 為的就是得到非資料集的答案 08/10 11:12

→ stosto      : 現在要限縮？ 08/10 11:12

→ stosto      : 現在終於懂為什麼當初唸書的時候一堆人np 問題的精 08/10 11:14

→ stosto      : 髓不會用 08/10 11:14

推 Isopod      : 哥 你該學新東西了 08/10 11:15

→ Isopod      : 基本上你講的沒錯，但不是大家正在討論的東西… 08/10 11:15

推 SiFox       : 不要看做人工智能AI，事實上就是資料彙整生成軟體 08/10 11:16

→ SiFox       : 這個資料生成軟體自由發揮的空間很高，甚至會幻想 08/10 11:16

→ SiFox       : 這樣就夠了~ 哈哈阿 08/10 11:16

→ csqeszzz    : 有個重點是，人一般會有自己可能說錯的自覺，也有 08/10 11:19

→ csqeszzz    : 意識到對某事缺乏了解的能力，對此一般人會承認自 08/10 11:19

→ csqeszzz    : 己的錯誤或無知，與之對話者就更能夠合作去尋求解 08/10 11:19

→ csqeszzz    : 答，但chatgpt缺乏這樣的自覺與承認的能力，它只會 08/10 11:19

→ csqeszzz    : 一直唬爛下去。這不是單純說錯話而已。 08/10 11:19

噓 josephroyal : 半瓶水響叮噹 08/10 11:20

推 Jiming      : 去修資工AI，會了解更多 08/10 11:20

→ stkoso      : 機率分布沒有明顯的峰值時就代表很有可能出錯了 08/10 11:23

→ stkoso      : 人要做的是根據結果來調整閾值 08/10 11:24

→ Mchord      : 哪有什麼幻覺不能解決的，就沒訓練好而已 08/10 11:27

推 stosto      : 很難有完美的模型，所以一定會有他們所謂的幻覺 08/10 11:28

→ laechan     : https://youtube.com/watch?v=AKqLiLLl8fo#t=180s 08/10 11:29

→ laechan     : 就算是真正的人也會虎爛你，你又怎麼判斷真偽呢? 08/10 11:29

推 kanx        : 也不是沒訓練好, 限制不能有幻覺, 就變成多元回歸了 08/10 11:30

推 ji3g4zo6    : stkoso說的是對的 不調整根本沒辦法避免幻覺 跟資 08/10 11:34

→ ji3g4zo6    : 料庫一點關係也沒有 08/10 11:34

→ afrazhao    : 講的超好，一堆人一直在吵有沒有錯誤，但憑什麼你認 08/10 11:36

→ afrazhao    : 為的錯誤就是大家的錯誤？這個對錯、完美是誰的標準 08/10 11:36

→ afrazhao    : 建立的，換了一個人來看是不是本來的正確跟完美又是 08/10 11:36

→ afrazhao    : 被充滿瑕疵？ 08/10 11:36

推 Isopod      : 看來樓上認為3顆原子彈是完美的答案呢 08/10 11:40

推 afrazhao    : 現在的幻覺是不是建立在AI變成一個討好人格？？ 不 08/10 11:41

→ afrazhao    : 急於事實提供建議.... 如果是的話這真的要認真去思 08/10 11:41

→ afrazhao    : 考有沒有可以改善的方式 08/10 11:41

→ GoalBased   : 幻覺和回答錯誤是兩回事，你說加法，數學問題，結果 08/10 11:41

→ GoalBased   : 不一樣就錯誤，但幻覺不是這個意思 08/10 11:41

→ IBIZA       : 以數學舉例的話 08/10 11:46

→ IBIZA       : 所謂的錯誤, 指的是四則運算的結果不對 08/10 11:46

→ IBIZA       : 幻覺的話是chatGPT告訴你有加減乘除以外的第五種 08/10 11:46

→ IBIZA       : 運算, 而且還用這種運算算出結果給你 08/10 11:47

推 artning     : AI的問題是表面效度太高了—即使他的答案根本就是瞎 08/10 11:51

→ artning     : 掰，仍然必須真的對這題本來就很了解的人才能辨識。 08/10 11:51

推 SiFox       : 自學習資料擴張生成軟體，這是目前AI真面目 08/10 11:52

→ GoalBased   : 你可以說是演算法不夠好或者資料來源不夠乾淨導致他 08/10 11:52

→ GoalBased   : 的回答”錯誤”，但這種錯誤對大部分的人來講就叫做 08/10 11:52

→ GoalBased   : 幻覺 08/10 11:52

→ SiFox       : 幫助幻想瞎掰，或合成圖像很好用 08/10 11:52

→ SiFox       : 幫忙去馬賽克也很有效率 08/10 11:53

推 SiFox       : 應用到對的產業，也是不錯啦 08/10 11:56

噓 la8day      : 比較好的解決方式是：不知道就說不知道 08/10 11:58

→ la8day      : 但他現在就做不到 08/10 11:58

→ SiFox       : 問題就再，如果說"不知道"也是在迎合人類 (說謊 08/10 11:59

→ SiFox       : 黑盒子存在AI就不可能發展，但可以拿來應用別的領域 08/10 12:00

推 ck8861103   : 三顆原子彈是因為你駁回他的回答啊 chatGPT不是wiki 08/10 12:01

→ SiFox       : 所以好玩就好，對AI別太認真 08/10 12:01

→ ck8861103   : AI哪管這是不是歷史事實 她只知道你不喜歡這答案 08/10 12:02

推 Isopod      : 用過chatgpt的會知道，一般情況下反駁他也沒用吧。 08/10 12:02

→ Isopod      : 至少我的使用經驗是這樣啦 08/10 12:02

推 a77520601   : 我也覺得你不太了解實際的狀況，他都自信但但的捏造 08/10 12:03

→ a77520601   : 出一些不存在的數據和細節、規則。很多的錯誤是有根 08/10 12:03

→ a77520601   : 據的、有因果或是有人為判斷的錯誤，可以檢討、修正 08/10 12:03

→ a77520601   : 。畢竟人類短時間還是很難把判斷交給AI，但如果今天 08/10 12:03

→ a77520601   : 你詢問GPT一些事情他會憑空的編造完全不存在的事並 08/10 12:03

→ a77520601   : 且說的非常合理，包含研究機構、報導、統計通通憑空 08/10 12:03

→ a77520601   : 編造，你得自己查證過一遍，任何使用GPT尋找根據的 08/10 12:03

→ a77520601   : 作法都是不可靠的，但Bing目前用起來這個問題就很少 08/10 12:03

→ strlen      : 我講完整一點 AI的問題其實要分成兩個層面 一個是 08/10 12:04

→ strlen      : 假資訊 一個是無中生有 後者會影響到前者 但後者卻 08/10 12:04

→ strlen      : 也同樣是目前大型生成式語言模型所追求的 如要希望 08/10 12:04

→ strlen      : 夠精準的資訊 就要降低無中生有的能力 但無中生有 08/10 12:04

→ strlen      : 卻正好是本次突破性的特色 不然其實只是想要一個能 08/10 12:04

→ strlen      : 用自然語言提問的百科全書 那ChatGPT也不會爆紅了 08/10 12:04

→ strlen      : 無中生有這個特點的確是兩面刃 但光是這樣 很多情 08/10 12:04

→ strlen      : 境下就已經非常好用了 新聞裡提出的問題是精準度問 08/10 12:04

→ strlen      : 題 這個其實不難解 08/10 12:04

→ potionx     : https://youtu.be/SOAbgV07IH8?t=227 08/10 12:04

噓 CYL009      : 看到第二段開頭就知道可以結束惹 你對AI 的認知還太 08/10 12:04

→ CYL009      : 少 08/10 12:04

→ potionx     : https://youtu.be/ORHv8yKAV2Q?t=1202 08/10 12:04

→ potionx     : 可以稍微上課一下 看看台大怎麼教學的 08/10 12:05

推 SiFox       : 別太糾結，我已經找到最佳應用領域 去馬賽克 08/10 12:06

→ SiFox       : 我可以接受去馬賽克後，不夠精準，但有像就夠了 08/10 12:07

→ SiFox       : 我相信大家都可以接受，手指頭稍微扭曲或變六隻 08/10 12:10

→ SiFox       : OK的 08/10 12:11

→ jasperhai   : 結果看來文組理組都看不下去XD 08/10 12:13

推 ptta        : 你完全理解錯誤 08/10 12:14

→ ptta        : 不是訓練資料的正確性的問題 08/10 12:15

→ ku399999    : 我就問你什麼科系學什麼的 哪來一堆推論 08/10 12:15

→ strlen      : 最後還是看你的需求 如果你今天要的是 問AI他就得 08/10 12:21

→ strlen      : 回答一個可靠精準不唬爛的答案 那你要的就是百科全 08/10 12:21

→ strlen      : 書 那至少現在ChatGPT無法 因為唬爛在某些情境之下 08/10 12:21

→ strlen      : 是很有用的 它設計取向包山包海 無法兼顧 我認為最 08/10 12:21

→ strlen      : 後目標是像bing AI使用者自行選精準度高的AI 或是 08/10 12:21

→ strlen      : 無中生有超愛唬爛的AI吧 08/10 12:21

推 a77520601   : 還有那個六位數加法會不一樣是真的，不只6位數，舉 08/10 12:24

→ a77520601   : 凡任何數理邏輯規則的演算都會亂講，1+1都會算錯。 08/10 12:24

→ a77520601   : 我曾試著訓練AI做會計，到後面真的矇了，全憑幻想 08/10 12:24

→ strlen      : 但講是這樣講 現階段當然是沒做到很好 所有人都還 08/10 12:26

→ strlen      : 在趕工調整 確實達不到需求 當然這也是股價還有想 08/10 12:26

→ strlen      : 像空間的理由 要是都發展得差不多了就是糕點喇 08/10 12:26

推 FCPEWN375   : 如何避免人類滅亡？ AI：監禁人類 08/10 12:26

→ strlen      : ChatGPT中的幻想 是特色之一啊 特別是創意類文案 08/10 12:27

→ strlen      : 所以今天就變成 跟創意相關產業人士就覺得神讚 但 08/10 12:29

→ strlen      : 想要百科全書的人就該該叫 合理啦 08/10 12:29

→ strlen      : 光是adobe的那個AI修圖補圖 真的超神超好用 08/10 12:30

推 Samurai     : AI不會無中生有，但是會從資料庫拼湊出不正確的資 08/10 12:30

→ Samurai     : 訊 08/10 12:30

推 chigo520    : 事實上亂產資料不是人類最愛幹的科科 應該反而要害 08/10 12:32

→ chigo520    : 怕 ai越來越人性化了 08/10 12:32

推 benjamin8312: AI 跟 公式推導本身就是不一樣的東西，AIV出來的東 08/10 12:35

→ benjamin8312: 西就不是100%一樣，是一個信心度的問題，現在就是AI 08/10 12:35

→ benjamin8312: 對原資料做了未知的剖析，根本不是參照資料有沒有假 08/10 12:35

→ benjamin8312: 的問題 08/10 12:35

推 sharline1013: 我是拿我自己整好的資料請他減少冗句和分段，就會發 08/10 12:35

→ sharline1013: 現他完全誤解其中一段的主題然後亂湊，但別人乍看真 08/10 12:35

→ sharline1013: 的看不出來 08/10 12:35

噓 manysheep   : 大家也太溫和 你根本整個搞不清狀況 還扯到哲學 08/10 12:41

→ niburger1001: gigo 08/10 12:43

→ manysheep   : 先搞懂甚麼是機器學習好嗎 一直是說是給程式的問題 08/10 12:43

→ manysheep   : 跟資料庫正確率無關 是機器學習後果就會產生的憑空 08/10 12:43

→ manysheep   : 虛造 跟人類會話唬爛一樣 跟你這篇會不懂裝懂硬扯一 08/10 12:44

→ manysheep   : 樣 08/10 12:44

→ strlen      : https://www.youtube.com/watch?v=vFRnBPfYuls 08/10 12:47

→ strlen      : 唬爛為主場的應用情境 08/10 12:47

→ philip81501 : 你商家數據資料 總要最新的為主吧 08/10 12:47

→ strlen      : 她回答不了 有差嗎 大家就是要看AI pon 08/10 12:49

推 mcroplet    : AI只能是工具，不能是人性 08/10 12:50

→ MinuteMan   : 人家就只是個LLM一直要他變人 08/10 12:57

推 a30332520   : 我問灌籃高手哪隊是全國冠軍 08/10 13:04

→ a30332520   : 他回答灌籃高手主角是流川楓 08/10 13:04

→ a30332520   : 湘北高中是全國冠軍，大言不慚的唬爛… 08/10 13:04

→ a30332520   : 然後我說湘北就打輸愛和了哪有全國冠軍，他又回答 08/10 13:07

→ a30332520   : 山王才是全國冠軍，這是不是幻覺 08/10 13:07

噓 adonisXD    : 就是有人故意餵不存在的錯誤資料啊 08/10 13:08

→ adonisXD    : 誰跟你幻覺 08/10 13:08

→ adonisXD    : 不信你去餵韭菜DPP清廉勤政愛鄉土 08/10 13:08

→ adonisXD    : 久了韭菜就回你DPP清廉勤政愛鄉土 08/10 13:08

→ a30332520   : 我又說山王在第二場就打輸湘北了 08/10 13:08

→ a30332520   : 他又回答全國冠軍是翔陽… 08/10 13:08

→ adonisXD    : 一樣道理啊 08/10 13:08

→ strlen      : https://i.imgur.com/6fa1FE8.png 你要是百科全書啊 08/10 13:08

→ a30332520   : 樓上，請問我問的網路上沒有一個資料顯示過翔陽拿 08/10 13:09

→ a30332520   : 過全國冠軍喔，這資料哪來的？ 08/10 13:09

→ strlen      : 只是要可用自然語言提問的百科全書 簡單很多 08/10 13:09

→ strlen      : 就是全部去尻搜尋資料比對 最多出現的 基本就是答案 08/10 13:10

→ a30332520   : 這就是所謂幻覺，如果再更深一點的問題，你甚至會 08/10 13:10

→ a30332520   : 相信AI，會不會成為災難就不知道了 08/10 13:10

→ strlen      : 但如果只是這樣 今天chatGPT就很無聊 也不會爆紅 08/10 13:10

→ strlen      : 對 是無中生有的幻想 但這就是他暴紅的特點 08/10 13:11

→ strlen      : 你需要的是精準的AI百科全書 ChatGPT目前無法做到 08/10 13:12

→ strlen      : 但我認為這只是設計取向不同而已 不算什麼大問題 08/10 13:12

→ strlen      : ChatGPT大概也不是以百科全書為目標的 08/10 13:14

推 swgun       : 引出不存在的判決書內容或字號就是錯的阿 怎麼取得 08/10 13:47

→ swgun       : 人類法官 08/10 13:47

→ swgun       :                        取代 08/10 13:47

推 imhanhan    : 好幾次誤導gpt,他竟然就開始跟著附和我XDD 08/10 13:57

推 SapiensChang: 能夠輔助人類使用的ai大概需要客製化啦 08/10 14:30

→ SapiensChang: 但必須要說 能夠達到輔助人類使用 覺得很堪用到有龐 08/10 14:30

→ SapiensChang: 大的商業價值 大概至少5年內不會發生 但我認為是10 08/10 14:30

→ SapiensChang: 年內 再來覺得幻覺能夠被解決這件事 就是外行人的幻 08/10 14:30

→ SapiensChang: 想而已 08/10 14:30

→ SapiensChang: 我發現很多ai迷信者都會在那堅信ai有天什麼事都會被 08/10 14:31

→ SapiensChang: 解決 但說真的就跟外星人被發現一樣現在就是莫名其 08/10 14:31

→ SapiensChang: 妙的低能論調 08/10 14:31

→ SapiensChang: 舉例來說 ai現在都無法判讀人類文字情緒了 連那麼基 08/10 14:33

→ SapiensChang: 本的事 ai都做不到 是要怎麼克服後面根本稍微難 但 08/10 14:33

→ SapiensChang: 對人類大概是呼吸等級的事 08/10 14:33

→ nrxadsl     : 沒哦，他最基本的連續十個亂數作加總平均都會錯哦， 08/10 14:58

→ nrxadsl     : 而且你還可以糾正他，拿錯的糾正他還會謝謝你 08/10 14:58

→ nrxadsl     : 其實你還可以第二次第三次糾正他，他到最後會反覆給 08/10 14:59

→ nrxadsl     : 你一樣的答案，直到最後他肯定的那個還是錯的，我有 08/10 15:00

→ nrxadsl     : 截圖下來 08/10 15:00

推 jamesho8743 : 先定義什麼叫AI幻覺吧 AI的答案本來就依照模型選出 08/10 15:15

→ jamesho8743 : 最高機率的回答的‘’近似‘’答案 本來就是在編造 08/10 15:15

噓 centaurjr   : 你根本不懂新聞在講啥吧,我今天問你床前明月光誰做 08/10 15:34

→ centaurjr   : 的,除了考試合理地回答應該是李白或是不知道選一個 08/10 15:35

→ centaurjr   : 你絕對不可能回答甚麼諸葛亮對吧...AI會 08/10 15:36

→ centaurjr   : 這就叫幻覺  懂?  資料庫沒有輸入他自己聯想的幻覺 08/10 15:36

推 jamesho8743 : chatgpt 本來就是超高級的文字接龍 高級到有時你覺 08/10 15:42

→ jamesho8743 : 得他有智慧 那是因為它是去近似訓練它的人類文本 08/10 15:42

→ jamesho8743 :  有時又覺得它在瞎扯 因為岳飛就是岳飛 少一個字都 08/10 15:42

→ jamesho8743 : 不行 08/10 15:42

推 kyo55966    : 我因為有在釣魚，曾經問過一些氣象紀錄，比如某地 08/10 15:43

→ kyo55966    : 歷史最高溫、日出時間、滿潮時間，而這些理論上都 08/10 15:43

→ kyo55966    : 能查到正確的紀錄，潮汐懂規律也可以推算，可是gpt 08/10 15:43

→ kyo55966    : 常常給我最新2023的數據，我問資料庫不是只有到2021 08/10 15:43

→ kyo55966    :  ？他就回對不起，2023不是紀錄不是推算，完全是瞎 08/10 15:43

→ kyo55966    : 掰…那會不會過去某一年其實也是瞎掰？問紀錄查不 08/10 15:43

→ kyo55966    : 到不是留白而是欺騙，這不是政治立場或人也會說謊 08/10 15:43

→ kyo55966    : 的問題，期待AI對這類基礎的問題回答正確且誠實應 08/10 15:43

→ kyo55966    : 該不算太高要求 08/10 15:43

推 kilhi       : AI幻覺是網軍問題 看看民進黨操作 08/10 16:10

→ blackstyles : 隨便問電影劇情就會產生幻覺了啦 笑死 有夠難用 08/10 17:02

噓 blackstyles : 問A答B 根本創作文了 笑死 08/10 17:06

→ blackstyles : https://i.imgur.com/x1p9uZ3.jpg 08/10 17:06

噓 k12151215   : 嗯 08/10 18:12

推 jamesho8743 : 你才不懂AI是啥吧?  AI哪來資料庫? 笑死人先去搞懂A 08/10 21:18

→ jamesho8743 : I是啥 你以為AI是靠資料庫來回答問題的嗎? 那資料庫 08/10 21:18

→ jamesho8743 : 要多大? AI只有訓練庫 訓練出來的是一個複雜函式權 08/10 21:18

→ jamesho8743 : 重 它是能回答任何問題的 亦即給定任何一輸入 它都 08/10 21:18

→ jamesho8743 : 會有輸出 它的輸出準確率本來就不是百分之百的  即 08/10 21:18

→ jamesho8743 : 使AlphaGo ChatGPT 用非常大量且高品質的資料訓練 08/10 21:18

→ jamesho8743 :  相比於整個空間還是非常微小 你問它床前明月光 它 08/10 21:18

→ jamesho8743 : 本來就可能會回答諸葛亮或任何人名 人類回答問題是 08/10 21:18

→ jamesho8743 : 靠正確理解 AI回答是靠機率 靠貼近它所訓練的資料 08/10 21:18

→ jamesho8743 :  它只是依照你的上下文選出一連串最可能的一個一個 08/10 21:18

→ jamesho8743 : 字 這本來就是瞎扯 本來就是幻覺 只是當大部分回答 08/10 21:19

→ jamesho8743 : 看起來都很順很合理時 你覺得沒問題 但是不合理的 08/10 21:19

→ jamesho8743 : 例子還是會很多 懂? 這時你就可以看出AI只是拼字 08/10 21:19

→ jamesho8743 :  而不是"真正"理解 但它依然實用 為何?當你逼近百 08/10 21:19

→ jamesho8743 : 分之九十九的九九時 不管它是不是真正理解 就行為 08/10 21:19

→ jamesho8743 : 來說跟真正理解也差不多了 08/10 21:19

推 jamesho8743 : 本文裡已經不就說的很清楚了嗎? 現在沒有任何一種 08/10 21:40

→ jamesho8743 : 模型能夠完全避免產生幻覺 因為它們的設計初衷只是 08/10 21:40

→ jamesho8743 : 用來預測下一個單詞 其實根本就沒有什麼"幻覺" 那是 08/10 21:40

→ jamesho8743 : 對一般外行人的說法 也沒有什麼編造答案 它本來就是 08/10 21:40

→ jamesho8743 : 在編答案只是有時對有時錯 08/10 21:40

推 jamesho8743 : 前面有人講的很對 所謂幻覺就只是錯誤率而已 幻覺這 08/10 21:54

→ jamesho8743 : 個詞是人類用法 這個錯誤率是不可避免的 一個是你訓 08/10 21:54

→ jamesho8743 : 練樣本有限 第二個是目前的演算法跟模型正確率還不 08/10 21:54

→ jamesho8743 : 夠高 08/10 21:54

推 jamesho8743 : 你問它原子彈丟幾顆? 它回答可以是3顆 2顆 5顆或任 08/10 22:12

→ jamesho8743 : 何常用的數字 這表明它就不是用什麼資料庫來回答 08/10 22:12

→ jamesho8743 :  而是每一次問它都可能會有些許不同 08/10 22:12

噓 bunjie      : 等等 你該自己花心力練習基本功的程式課 自己不努 08/27 12:47

→ bunjie      : 力找chatGPT代勞 這樣不太好吧 08/27 12:47

→ bunjie      : 另外重點在於不是所有東西都是chatGPT能勝任的 有 08/27 12:48

→ bunjie      : 的領域就是一定要百分百正確 才不會造成危險 所以A 08/27 12:48

→ bunjie      : I只是工具 chatGPT只是其中一種AI 08/27 12:48
