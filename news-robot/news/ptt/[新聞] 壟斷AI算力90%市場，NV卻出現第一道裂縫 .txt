作者MA40433 (Masa)看板Stock標題[新聞] 壟斷AI算力90%市場，NV卻出現第一道裂縫時間Sun May 21 20:29:12 2023
原文標題：
深度好文 | 壟斷AI算力90%的市場，英偉達帝國卻已出現第一道裂縫

原文連結：
來源富途 以下已經轉繁體
https://reurl.cc/QXnd82

發布時間：
2023/5/21 11:42

記者署名：
遠川研究所

原文內容：

儘管英偉達目前憑藉GPU+NVlink+CUDA壟斷了AI算力90%的市場，但帝國已經出現了第一道
裂縫。

2012年，AI圈發生了兩件大事，按時間順序，第一件是谷歌組團已久的Google Brain發佈
“出道作”——一個能夠識別貓的深度學習網路“谷歌貓”，74.8%的識別準確率，比知
名識別圖像大賽ImageNet前一年獲勝演算法的74%還要高出0.8%。

但谷歌的高光時刻只持續了幾個月。2012年12月，最新一屆ImageNet的獲勝者出爐，深度
學習大神Hinton及其弟子帶著卷積神經網路AlexNet，將識別正確率一舉提高到了84%，由
此開啟了之後十年的AI革命，谷歌貓則被埋進了歷史的塵埃之中。

讓業內震驚的不只是ImageNet模型本身。這個需要1400萬張圖片、總計262千萬億次浮點
運算訓練的神經網路，一個星期的訓練過程中僅用了四顆英偉達Geforce GTX 580。作為
參考，谷歌貓用了1000萬張圖片、16000顆CPU、1000台電腦[1]。

傳言Google在這一年也秘密參加了比賽，其受到的震撼直接體現在接下來的行動上：
Google一邊豪擲了4400萬美元收購了Hinton團隊，一邊馬上向英偉達下單大量GPU用來人
工智慧訓練，而且同時“掃貨”的還有微軟、Facebook等一眾巨頭。


英偉達成為最大的贏家，股價在接下10年裡最高漲了121倍。一個帝國誕生了。

但帝國的上空，逐漸聚攏了兩朵烏雲。當年向英偉達掃貨的Google，在三年後攜AlphaGo
驚豔亮相，並在2017年擊敗了人類冠軍柯潔。敏銳的人發現，驅動AlphaGo的晶片不再是
英偉達的GPU，而是Google自研的TPU晶片。

再過三年，相似劇情重演。曾經被黃仁勳一度視為標杆客戶的特斯拉也告別英偉達GPU，
先是推出了以NPU為核心的FSD車載晶片，然後又拿出了用來搭建AI訓練集群的D1晶片——
這意味著英偉達接連裡失去了AI時代裡兩個最重要的客戶。

到了2022年，全球IT週期進入下行階段，雲計算大廠紛紛削減資料中心的GPU採購預算，
區塊鏈挖礦大潮也逐漸冷卻等原因，英偉達庫存暴增，股價從最高點一度跌去了2/3。

2022年底ChatGPT橫空出世，GPU作為大模型“煉丹”的燃料再次遭到哄搶，英偉達獲得喘
息，但第三朵烏雲隨之而來：2023年4月18號，著名科技媒體The Information爆料：本輪
AI浪潮的發起者微軟，正在秘密研發自己的AI晶片[2]。

這款名叫Athena的晶片由台積電代工，採用5nm先進制程，微軟研發團隊人數已經接近300
人。很明顯，這款晶片目標就是替代昂貴的A100/H100，給OpenAI提供算力引擎，並最終
一定會通過微軟的Azure雲服務來搶奪英偉達的蛋糕。

微軟目前是英偉達H100最大的採購方，甚至一度傳出要“包圓”H100全年的產能。來自微
軟的分手信號無疑是一道晴天霹靂，要知道，即使在Intel最灰暗的時候，其客戶也沒有
一家“敢於”自造CPU晶片（除了蘋果，但蘋果並不對外銷售）。

儘管英偉達目前憑藉GPU+NVlink+CUDA壟斷了AI算力90%的市場，但帝國已經出現了第一道
裂縫。

1、本不為AI而生的GPU
打從一開始，GPU就不是為AI所生。

1999年10月英偉達發佈了GeForce 256，這是一款基於台積電220納米工藝、集成了2300萬
個電晶體的圖形處理晶片。英偉達把Graphics Processing Unit的首字母「GPU」提煉出
來，把GeForce 256冠以“世界上第一塊GPU”稱號，巧妙地定義了GPU這個新品類，並佔
據這個詞的用戶心智直到今天。

而此時人工智慧已經沉寂多年，尤其是深度神經網路領域，GeofferyHinton和YannLeCun
等未來的圖靈獎獲得者們還在學術的冷板凳上坐著，他們萬萬不會想到自己的職業生涯，
會被一塊本來為遊戲玩家開發的GPU所徹底改變。

GPU為誰所生？圖像。更準確地說，是為CPU從圖像顯示的苦力活中解放出來而生。圖像顯
示的基本原理是將每一幀的圖像分割成一顆顆圖元，再對其進行頂點處理，圖元處理，柵
格化、片段處理、圖元操作等多個渲染處理，最終得以顯示在螢幕上。

https://i.imgur.com/czFR9wV.jpg
 從圖元到圖像的處理過程 圖源：graphics compendium

為什麼說這是苦力活呢？做一個簡單的算術題：

假定螢幕上有30萬顆圖元，以60fps幀率計算，每秒需要完成1800萬次渲染，每次包含上
述五個步驟，對應五條指令，也就是說，CPU每秒要完成9000萬條指令才能實現一秒的畫
面呈現，作為參考，當時英特爾性能最高的CPU每秒算力才6000萬次。

不怪CPU弱，而是其本就以執行緒調度見長，為此將更多的空間讓渡給了控制單元和存儲
單元，用於計算的計算單元只佔據20%的空間。GPU則相反，80%以上空間是計算單元，帶
來了超強平行計算能力，更適合圖片顯示這種步驟固定、重複枯燥的工作。

https://i.imgur.com/uqzwXSk.jpg
 CPU和GPU內部結構，綠色部分為運算單元

直到幾年後，一些人工智慧學者才意識到，具備這樣特性的GPU也適用於深度學習的訓練
。很多經典的深度神經網路架構早在20世紀下半葉就已經被提出，但因為缺乏訓練它們的
計算硬體，很多研究只能“紙上談兵”，發展長期停滯。

1999年10月的一聲炮響，給人工智慧送來了GPU。深度學習的訓練過程是對每個輸入值根
據神經網路每層的函數和參數進行分層運算，最終得到一個輸出值，跟圖形渲染一樣都需
要大量的矩陣運算——這恰巧就是GPU最擅長的東西。

https://i.imgur.com/blom4S9.jpg
 一個典型的深度神經網路架構；圖源：towards data science

不過圖像顯示雖然資料處理量龐大，但大部分步驟是固定的，而深度神經網路一旦運用至
決策領域，會涉及到分支結構等複雜情況，每層的參數又需要基於海量資料正負反饋訓練
來不斷修正。這些差別為日後GPU對於AI的適應性埋下了隱患。
如今的亞馬遜AI/ML總經理Kumar Chellapilla是最早吃到GPU螃蟹的學者。2006年他使用
英偉達的GeForce 7800顯卡第一次實現了卷積神經網路（CNN），發現比使用CPU要快4倍
。這是已知最早將GPU用於深度學習的嘗試[3]。

Kumar的工作並未引起廣泛的注意，很重要的原因是基於GPU編寫程式的複雜度很高。但恰
在此時，英偉達於2007年推出了CUDA平臺，開發者利用GPU來訓練深度神經網路的難度大
幅度降低，這讓深度學習教徒們看到了更多希望。

隨後便是2009年，斯坦福的吳恩達等人發表了突破性的一篇論文[6]，GPU憑藉超過CPU 70
倍的算力將AI訓練時間從幾周縮短到了幾小時。這篇論文為人工智慧的硬體實現指明了方
向。GPU大大加速了AI從論文走向現實的過程。

經過無數人的探索，接力棒終於交到了深度學習大師Hinton的手上，此時時間已經指向了
2012年。

2012年，Hinton和Alex Krizhevsky、Ilya Sutskeverz這兩位學生一起設計了一個深度卷
積神經網路AlexNet，計畫參加這一年的ImageNet大賽。但問題是如果用CPU來訓練
AlexNet可能需要幾個月的時間，於是他們把目光轉向了GPU。

這顆在深度學習的發展歷史中至關重要的GPU，便是著名的“核彈顯卡”GTX 580。作為英
偉達最新Fermi架構的旗艦產品，GTX 580被塞入512顆CUDA核心（上一代為108顆），算力
飛躍的同時，誇張的功耗和發熱問題也讓英偉達被賜名“核彈工廠”。

甲之砒霜，乙之蜜糖。跟用GPU訓練神經網路時的“順滑”相比，散熱問題簡直不值一提
。Hinton團隊用英偉達的CUDA平臺順利地完成了程式設計，在兩張GTX 580顯卡的支持下
，1400萬張圖片的訓練只花了一個周，AlexNet順利奪冠。

由於ImageNet比賽和Hinton本人的影響力，所有人工智慧學者都在一瞬間意識到了GPU的
重要性。

兩年後，谷歌攜GoogLeNet模型參加ImageNet，以93%的準確率奪冠，採用的正是英偉達
GPU，這一年所有參賽團隊GPU的使用數量飆升到了110塊。在比賽之外，GPU已經成為深度
學習的“必選消費”，給黃仁勳送來源源不斷的訂單。

這讓英偉達擺脫了移動端市場慘敗的陰影——2007年iPhone發佈後，智慧手機晶片的蛋糕
迅速膨脹，英偉達也試圖從三星、高通、聯發科等碗裡分一杯羹，但推出的Tegra處理器
因為散熱問題鎩羽而歸。最後反而是被GPU拯救的人工智慧領域，反哺給了英偉達一條第
二增長曲線。

但GPU畢竟不是為了訓練神經網路而生，人工智慧發展得越快，這些問題暴露得就越多。
例如，雖然GPU跟CPU差異顯著，但兩者根子上都遵循馮‧諾伊曼結構，存儲和運算是分離
的。這種分離帶來的效率瓶頸，影像處理畢竟步驟相對固定，可以通過更多的並行運算來
解決，但在分支結構眾多的神經網路中很是要命。

神經網路每增加一層或一個分支，就要增加一次記憶體的訪問，存儲資料以供回溯，花費
在這上面的時間不可避免。尤其在大模型時代，模型越大需要執行的記憶體訪問操作就越
多——最後消耗在記憶體訪問上的能耗要遠比運算要高很多倍。

簡單比喻就是，GPU是一個肌肉發達（計算單元眾多）的猛男，但對於收到的每條指令，
都得回過頭去翻指導手冊（記憶體），最後隨著模型大小和複雜度的提升，猛男真正幹活
的時間很有限，反而被頻繁地翻手冊累到口吐白沫。

記憶體問題只是GPU在深度神經網路應用中的諸多“不適”之一。英偉達從一開始就意識
到這些問題，迅速著手“魔改”GPU，讓其更適應人工智慧應用場景；而洞若觀火的AI玩
家們也在暗渡陳倉，試圖利用GPU的缺陷來撬開黃仁勳帝國的牆角。

一場攻防戰就開始了。


02、Google和Nvidia的暗戰

面對排山倒海的AI算力需求和GPU的先天缺陷，黃仁勳祭出兩套應對方案，齊頭並進。
第一套，就是沿著“算力老仙，法力無邊”的路子，繼續暴力堆砌算力。在AI算力需求每
隔3.5個月就翻倍的時代，算力就是吊在人工智慧公司眼前的那根胡蘿蔔，讓他們一邊痛
駡黃仁勳的刀法精湛，一邊像舔狗一樣搶光英偉達所有的產能。

第二套，則是通過“改良式創新”，來逐步解決GPU跟人工智慧場景的不匹配問題。這些
問題包括但不限於功耗、記憶體牆、頻寬瓶頸、低精度計算、高速連接、特定模型優化…
…從2012年開始，英偉達驟然加快了架構更新的速度。

英偉達發佈CUDA後，用統一的架構來支撐Graphics和Computing這兩大場景。2007年第一
代架構登場，取名Tesla，這並非是黃仁勳想示好馬斯克，而是致敬物理學家尼古拉‧特
斯拉（最早還有一代是居裡架構）。

之後，英偉達每一代GPU架構都以著名科學家來命名，如下圖所示。在每一次的架構反覆
運算中，英偉達一邊繼續堆算力，一邊在不“傷筋動骨”的前提下改良。

https://i.imgur.com/UqpHHnx.jpg

比如2011年的第二代Fermi架構，缺點是散熱拉胯，而2012年的第三代架構Kepler就把整
體設計思路從high-perfermance轉向power-efficient，改善散熱問題；而為了解決前文
提到的“肌肉傻瓜”的問題，2014年的第四代Maxwell架構又在內部增加更多的邏輯控制
電路，便於精准控制。

為了適應AI場景，英偉達“魔改”後的GPU某種程度上越來越像CPU——正如CPU優秀的調
度能力是以犧牲算力為代價一樣，英偉達不得不在計算核心的堆疊上克制起來。但身背通
用性包袱的GPU再怎麼改，在AI場景下也難敵專用晶片。

率先對英偉達發難的，是最早大規模採購GPU來進行AI計算的Google。

2014年憑藉GoogLeNet秀完肌肉後，Google就不再公開參加機器識別大賽，並密謀研發AI
專用晶片。2016年Google憑藉AlphaGo先聲奪人，贏下李世石後旋即推出自研的AI晶片TPU
，以“為AI而生”的全新架構打了英偉達一個措手不及。

TPU是Tensor Processing Unit的首字母縮寫，中文名叫做“張量處理單元”。如果說英
偉達對GPU的“魔改”是拆了東牆補西牆，那麼TPU便是通過從根本上大幅降低存儲和連接
的需求，將晶片空間最大程度讓渡給了計算，具體來說兩大手段：

第一是量化技術。現代電腦運算通常使用高精度資料，佔用記憶體較多，但事實上在神經
網路計算大多不需要精度達到32位元或16位浮點計算，量化技術的本質基本上是將32位元
/16位元數位近似到8位元整數，保持適當的準確度，降低對存儲的需求。

第二是脈動陣列，即矩陣乘法陣列，這也是TPU與GPU最關鍵的區別之一。簡單來說，神經
網路運算需要進行大量矩陣運算，GPU只能按部就班將矩陣計算拆解成多個向量的計算，
每完成一組都需訪問記憶體，保存這一層的結果，直到完成所有向量計算，再將每層結果
組合得到輸出值。

而在TPU中，成千上萬個計算單元被直接連接起來形成矩陣乘法陣列，作為計算核心，可
以直接進行矩陣計算，除了最開始從載入資料和函數外無需再訪問存儲單元，大大降低了
訪問頻率，使得TPU的計算速度大大加快，能耗和物理空間佔用也大大降低。

https://i.imgur.com/GzFHISY.jpg
 CPU、GPU、TPU記憶體（memory）訪問次數對比

Google搞TPU速度非常快，從設計、驗證、量產到最後部署進自家資料中心只花了15個月
的時間。經過測試，TPU在CNN、LSTM、MLP等AI場景下的性能和功耗大大勝過了英偉達同
期的GPU。壓力便一下子全部給到了英偉達。

被大客戶背刺的滋味不好受，但英偉達不會站著挨打，一場拉鋸戰開始了。

Google推出TPU的5個月後，英偉達也祭出了16nm工藝的Pascal架構。新架構一方面引入了
著名的NVLink高速雙向互聯技術，大幅提升連接頻寬；一方面模仿TPU的量化技術，通過
降低資料精度來提升神經網路的計算效率。

2017年，英偉達又推出了首個專為深度學習設計的架構Volta，裡面第一次引入了
TensorCore，專門用於矩陣運算的——雖然4×4的乘法陣列跟TPU 256×256的脈動陣列相
比略顯寒酸，但也是在保持靈活和通用性的基礎上作出的妥協。

https://i.imgur.com/LcALQvM.jpg
 在英偉達V100中TensorCore實現的4x4矩陣運算

英偉達的高管對客戶宣稱：“Volta並不是Pascal的升級，而是一個全新的架構。”

Google也分秒必爭，2016年以後TPU在五年內更新了3代，2017年推出了TPUv2、2018年推
出了TPUv3、2021年推出了TPUv4，並把資料懟到英偉達的臉上[4]：TPU v4比英偉達的
A100計算速度快1.2～1.7倍，同時功耗降低1.3～1.9倍。

Google並不對外出售TPU晶片，同時繼續大批量採購英偉達的GPU，這讓兩者的AI晶片競賽
停留在“暗鬥”而非“明爭”上。但畢竟Google把TPU其部署到自家的雲服務系統中，對
外提供AI算力服務，這無疑壓縮了英偉達的潛在市場。

在兩者“暗鬥”的同時，人工智慧領域的進展也一日千里。2017年Google提出了革命性的
Transformer模型，OpenAI隨即基於Transformer開發了GPT-1，大模型的軍備競賽爆發，
AI算力需求自2012年AlexNet出現之後，迎來了第二次加速。

察覺到新的風向之後，英偉達在2022年推出Hopper架構，首次在硬體層面引入了
Transformer加速引擎，宣稱可以將基於Transformer的大語言模型的訓練時間提升9倍。
基於Hopper架構，英偉達推出了“地表最強GPU”——H100。

H100是英偉達的終極“縫合怪”，一方面引入了各種AI優化技術，如量化、矩陣計算（
Tensor Core 4.0）和Transformer加速引擎；另一方面則堆滿了英偉達傳統強項，如7296
個CUDA核、80GB的HBM2顯存以及高達900GB/s的NVLink 4.0連接技術。

手握H100，英偉達暫時松一口氣，市面上尚未出現比H100更能打的量產晶片。

Google和英偉達的暗中拉鋸，同樣也是是一種相互成就：英偉達從Google舶來了不少創新
技術，Google的人工智慧前沿研究也充分受益于英偉達GPU的推陳出新，兩者聯手把AI算
力降低到大語言模型“踮著腳”能用得起的水準。風頭正勁者如OpenAI，也是站在這兩位
的肩膀之上。

但情懷歸情懷，生意歸生意。圍繞GPU的攻防大戰，讓業界更加確定了一件事情：GPU不是
AI的最優解，定制化專用晶片（ASIC）有破解英偉達壟斷地位的可能性。裂縫已開，循味
而來的自然不會只有Google一家。

尤其是算力成為AGI時代最確定的需求，誰都想吃飯的時候跟英偉達坐一桌。


3、一道正在擴大的裂縫

本輪AI熱潮除了OpenAI外，還有兩家出圈的公司，一家是AI繪圖公司Midjourney，其對各
種畫風的駕馭能力讓無數碳基美工心驚膽戰；另外一家是Authropic，創始人來自OpenAI
，其對話機器人Claude跟ChatGPT打得有來有回。

但這兩家公司都沒有購買英偉達GPU搭建超算，而是使用Google的算力服務。
為了迎接AI算力的爆發，Google用4096塊TPU搭建了一套超算（TPU v4 Pod），晶片之間
用自研的光電路開關（OCS）互連，不僅可以用來訓練自家的LaMDA、MUM和PaLM等大語言
模型，還能給AI初創公司提供價廉物美的服務。

https://i.imgur.com/uf0Uo9U.jpg
 Google TPU v4 Pod超算

自己DIY超算的還有特斯拉。在推出車載FSD晶片之後，特斯拉在2021年8月向外界展示了
用3000塊自家D1晶片搭建的超算Dojo ExaPOD。其中D1晶片由台積電代工，採用7nm工藝，
3000塊D1晶片直接讓Dojo成為全球第五大算力規模的電腦。

不過兩者加起來，都比不過微軟自研Athena晶片所帶來的衝擊。

微軟是英偉達最大的客戶之一，其自家的Azure雲服務至少購買了數萬張A100和H100高端
GPU，未來不僅要支撐ChatGPT天量的對話消耗，還要供給Bing、Microsoft 365、Teams、
Github、SwiftKey等一系列要使用AI的產品中去。

仔細算下來，微軟要繳納的“Nvidia稅”是一個天文數字，自研晶片幾乎是必然。就像阿
裡當年算了一下淘寶天貓未來對雲計算、資料庫、存儲的需求，發現也是一個天文數字，
於是果斷開始扶持阿裡雲，內部展開轟轟烈烈的“去IOE”運動。

節省成本是一方面，垂直整合打造差異化是另一方面。在手機時代，三星手機的CPU（AP
）、記憶體和螢幕都是自產自銷，為三星做到全球安卓霸主立下汗馬功勞。Google和微軟
造芯，也是針對自家雲服務來進行晶片級優化，打造差異性。

所以，跟蘋果三星不對外出售晶片不同，Google和微軟的AI晶片雖然也不會對外出售，但
會通過“AI算力雲服務”來消化掉英偉達一部分潛在客戶，Midjourney和Authropic就是
例子，未來會有更多的小公司（尤其是AI應用層）選擇雲服務。

全球雲計算市場的集中度很高，前五大廠商（亞馬遜AWS、微軟Azure、Google Cloud、阿
裡雲和IBM）占比超60%，都在做自己的AI晶片，其中Google的進度最快、IBM的儲備最強
、微軟的衝擊最大、亞馬遜的保密做得最好、阿裡做的困難最多。

國內大廠自研晶片，Oppo哲庫的結局會給每個入場的玩家投上陰影。但海外大廠做自研，
人才技術供應鏈都可以用資金來構建出來，比如特斯拉當年搞FSD，挖來了矽谷大神Jim
Keller，而Google研發TPU，直接請到了圖靈獎獲得者、RISC架構發明人David Patterson
教授。

https://i.imgur.com/UKMrpQ1.jpg

除了大廠外，一些中小公司也在試圖分走英偉達的蛋糕，如估值一度達到28億美金的
Graphcore，國內的寒武紀也屬於此列。上表列舉了目前全球範圍內較為知名的初創AI晶
片設計公司。

AI晶片初創公司的困難在於：沒有大廠雄厚的財力持續投入，也不能像Google那樣自產自
銷，除非技術路線獨闢蹊徑或者優勢特別強悍，否則在跟英偉達短兵相接時基本毫無勝算
，後者的成本和生態優勢幾乎可以抹平客戶一切疑慮。

Start-up公司對英偉達的衝擊有限，黃仁勳的隱憂還是在那些身體不老實的大客戶身上。
當然，大廠現在還離不開英偉達。比如即使Google的TPU已經更新到了第4代，但仍然需要
大批量採購GPU來跟TPU協同提供算力；特斯拉即使有了性能吹上天的Dojo超算，馬斯克在
籌建AI新公司時仍然選擇向英偉達採購10000張GPU。

不過對於大廠的塑膠友情，黃仁勳早就在馬斯克身上領略過。2018年馬斯克公開宣稱要自
研車載晶片（當時用的是英偉達的DRIVE PX），黃仁勳在電話會議上被分析師當場質問，
一度下不來台。事後馬斯克發表了一番“澄清”，但一年之後特斯拉仍然頭也不回地離英
偉達而去[5]。

大廠在省成本這方面，從來不會留情。PC機時代Intel的晶片雖然賣給B端，但消費者具有
強烈的選擇自主性，廠商需要標榜“Intel Inside”；但在算力雲化時代，巨頭可以遮罩
掉一切底層硬體資訊，未來同樣購買100TFlops算力，消費者能分得清哪部分來自TPU，哪
部分來自GPU嗎？

因此，英偉達最終還是要直面那個問題：GPU的確不是為AI而生，但GPU會不會是AI的最優
解？

17年來，黃仁勳把GPU從單一的遊戲和影像處理場景中剝離出來，使其成為一種通用算力
工具，礦潮來了抓礦潮，元宇宙火了跟元宇宙、AI來了抱AI，針對一個個新場景不斷“魔
改”GPU，試圖在“通用性”和“專用性”之間找到一個平衡點。

複盤英偉達過去二十年，其推出了數不清的改變業界的新技術：CUDA平臺、TensorCore、
RT Core（光線追蹤）、NVLink、cuLitho平臺（計算光刻）、混合精度、Omniverse、
Transformer引擎……這些技術説明英偉達從一個二線晶片公司變成了全行業市值的南波
腕，不可謂不勵志。

但一個時代應該有一個時代的計算架構，人工智慧的發展一日千里，技術突破快到以小時
來計，如果想讓AI對人類生活的滲透像PC機/智慧手機普及時那樣大幅提升，那麼算力成
本可能需要下降99%，GPU的確可能不是唯一的答案。

歷史告訴我們，再如日中天的帝國，可能也要當心那道不起眼的裂縫。


心得/評論:
MS預計推出Athena 在Azure使用
Google預計在自己雲計算使用TPU
Amazon 則是在去年七月推出EC2

三大CSP都是不願意再去買貴森森的NV顯卡才去研發自己的晶片
只是不知道這些特規的晶片未來會不會流入市場被小家的CSP撿走


→ Alwen       : 說真的啦 ....2018年估狗TPU震天價響 05/21 20:32

→ Alwen       : 結果差不多已經失敗，市場繼續用老黃的 05/21 20:32

推 tomdavis    : 太長了 所有資金平均分給所有參賽者不就穩贏了 05/21 20:33

→ BlueBird5566: 到底 為什麼要叫英偉達 05/21 20:36

推 mamorui     : 其實看那麼多要跟AI扯關係 題材轉移的浮木不多 05/21 20:36

推 jack1218    : 兩朵烏雲XD 致敬物理學界嗎 05/21 20:40

噓 IBIZA       : 廢文一篇 無法做到壟斷就是裂縫 那有甚麼地方沒裂縫 05/21 20:40

→ Alwen       : 台積電裂縫也很大  慘惹 05/21 20:41

推 Swave       : 不管誰研發，通通都要台積做 05/21 20:43

推 kilhi       : 台GG最大贏家 AI下一階段是SIRI 05/21 20:43

→ Alwen       : 誰說的  市場一堆三星搶單的傳聞 裂縫大的 05/21 20:43

推 pandp       : 反正NV就軍火商，你做不出更好的也只能用他的 05/21 20:44

推 poeoe       : 微軟繼續持有就對了 讚啦 05/21 20:44

推 imhuck      : 來自對岸文章 一定長加上廢言一堆，然後結論虎頭蛇 05/21 20:44

→ imhuck      : 尾 05/21 20:44

推 kirry       : No殺手要出來了嗎 05/21 20:46

推 hcwang1126  : 特規都是軟硬整合才有搞頭 有了ChatGPT 微軟才開始 05/21 20:48

→ hcwang1126  : 搞自研 05/21 20:48

噓 hydra7      : 重點是什麼？ 05/21 20:48

推 maxbob      : 台積電要裂什麼縫？哀鳳還是無縫？ 05/21 20:49

推 Chilloutt   : 故事很棒，可以出書 05/21 20:52

→ azhu        : 廢話太多 結論是什麼 05/21 20:52

推 stanleyiane : 抬轎文太明顯了吧... 05/21 20:54

→ zerro7      : 自己研發晶片就一定會成功嗎...?? 05/21 20:54

推 maxbob      : 還在市場傳、恐三星搶單，真的笑死了 05/21 20:54

推 joygo       : 要不要寫恐整個ai團隊出走比較可怕？ 05/21 20:55

噓 hihi29      : ========建議還沒看文章的別浪費時間看了========== 05/21 20:57

→ bnn         : 兩朵烏雲XD 05/21 20:57

推 nidhogg     : 這種中國廢話文有夠多 05/21 20:58

→ Atwo        : 說的好 我大AMD 05/21 21:05

推 Brioni      : 結論就是TSMC或最贏 05/21 21:08

→ Brioni      : 管你是手機、汽車、雲端，通通來繳GG稅 05/21 21:09

噓 ck326       : 不意外ㄅ，現在每家巨頭都在自己搞 05/21 21:11

→ ck326       : 有什麼不滿就去怪aapl，它們家先這樣做的 05/21 21:12

推 Brioni      : 不過水果是去搞cpu，而且是消費電子市場 05/21 21:16

→ JuiFu617    : 怎麼看都是gg利多，你soc研發燒錢失敗，也還是給gg 05/21 21:17

→ JuiFu617    : 和創意錢 05/21 21:17

噓 adairchang  : 才想說一堆沒營養的情境用詞 一看果然是中國廢文 05/21 21:17

→ JuiFu617    : 而且量和體沒那麼多，有那麼好捆綁舊製程和新製程 05/21 21:18

→ JuiFu617    : 來議價？ 05/21 21:18

推 poeoe       : 真的AI搞起來 就是全世界繳錢給這些企業 05/21 21:19

→ poeoe       : 跟蘋果一樣 全世界收錢 05/21 21:19

→ Shepherd1987: GG訂價策略跟恐龍一樣 05/21 21:20

→ Shepherd1987: 都已經沒對手了, 麻煩跟老黃學學刀法好嗎 05/21 21:20

推 KadourZiani : 裂縫的深V是給你上車的 05/21 21:24

推 tsgd        : 農場文章 又臭又長 05/21 21:26

→ fallinlove15: 太耗電 沒有商業價值 05/21 21:26

推 f99999993   : 廢文一篇 05/21 21:27

推 luche       : 安全帶綁好 這是格雷的五十道陰影 05/21 21:28

噓 SaintsRow   : 廢到笑 05/21 21:28

→ mcucte      : 好長一篇，不想看 05/21 21:31

→ appledick   : 沒用啦 ChatGTP已經搶得先機了 大家已經習慣 除非出 05/21 21:33

→ appledick   : 現比ChatGTP更突破性的應用 05/21 21:33

噓 jack1218    : GPT啦 05/21 21:33

推 asdfg5678   : 爛文章... 05/21 21:39

→ AndyMAX     : 三星搶單喔 對啦 google pixels tensor找三星 TPU 05/21 21:39

→ AndyMAX     : 找台積 05/21 21:39

推 bj45566     : 反正 AI 大戰從演算法、Training、軟體，到晶片設計 05/21 21:46

→ bj45566     : 都不干台灣的事，台灣產業會活的比智慧手機時代和 05/21 21:46

→ bj45566     : 之前的 PC/NB 時代更艱辛 05/21 21:46

噓 Qnnnnn      : 廢文，可以講重點嗎？ 05/21 21:48

推 wei9898     : 好啦，不管哪家勝出，GG都是贏家，我買GG就好了 05/21 21:49

推 metallolly  : 太長了 05/21 22:00

→ shiyangfoo  : 依中國碼農的功力 如果GPU沒有限制出口..贏家很難說 05/21 22:01

推 bj45566     : 錯，中國的自然語言處理相關科技撐不起來 05/21 22:08

推 cychi       : 東拼西湊的文 05/21 22:08

推 f1r9a9n2k   : 這篇的意思就是噴 05/21 22:08

→ rkilo       : 都佔了90%市占，還裂縫... 05/21 22:09

推 bj45566     : LLM 這些仍然是西方創造的東西 05/21 22:10

推 good5755    : AI大戰目前看起來是美國全勝 不管最後誰贏都是美企 05/21 22:15

推 good5755    : 唯一希望的阿里巴巴是開曼群島企業 大股東是軟銀 05/21 22:18

→ good5755    : 笑死 05/21 22:18

推 CTTSAI      : 好拉東湊西湊 講重點很難？哪個裂縫 很懂三分鐘就點 05/21 22:19

→ CTTSAI      : 出前後問題在哪 05/21 22:19

→ baka1412    : 啥小廢文 05/21 22:20

→ acolam      : 講了老半天根本看不懂在講什麼 05/21 22:22

推 shinyi444   : 壓低吃貨 利空出盡 很有fu糗 藍買笑 05/21 22:24

推 bj45566     : 沒有喔，現在講 AI 基本上都包含 ML, PR；中國技術 05/21 22:29

→ bj45566     : 上最有競爭力的 AI 相關公司不是阿里巴巴，而是做電 05/21 22:29

→ bj45566     : 腦視覺和模式辨認為主的商湯科技和商湯科技 -- 不過 05/21 22:29

→ bj45566     : 這兩家公司都是美國提升晶片封鎖可以打爛的對象 05/21 22:29

→ bj45566     : 打錯：和曠視科技 05/21 22:30

推 nobit       : 丟這篇給ai擷取重點 05/21 22:33

→ nobit       : 結論ai還是fu糗無限 05/21 22:34

噓 kougousei   : 這篇到底想表達什麼？ 05/21 22:47

推 pantani     : 不要看結論  內容豐富  寫得不錯啊 05/21 22:53

推 strlen      : 這篇是不是AI寫的？這種文章的作者就是該被AI取代 05/21 22:57

→ ttsieg      : 誰能三行寫出重點？ 05/21 22:58

→ strlen      : 通篇就是看消息面自我腦補幻想勒 這個ChatGPT最擅長 05/21 22:58

推 Delisaac    : 中國人寫的文章就是這樣，又臭又長故弄玄虛，然後 05/21 22:59

→ Delisaac    : 看完發現其實也沒什麼創見 05/21 22:59

推 prtscscroll : 重點就是台積電或成最大贏家 05/21 23:00

推 aegis43210  : 在chatGPT出現後，AI的蛋糕變超大，老黃就算市佔下 05/21 23:05

→ aegis43210  : 滑也是大賺，這是去年預期不到的 05/21 23:05

推 PureTrue    : 很多公司都是說的一嘴好U 真的拿來比跟屎一樣 05/21 23:06

→ aegis43210  : 現在在AI訓練能和老黃拚的就蘇嬤以及2025的i皇 05/21 23:07

→ aegis43210  : GPGPU難度太高，能玩的沒幾家，其他只能去爭AI推理 05/21 23:09

→ aegis43210  : 的紅海市場 05/21 23:09

→ brain9453   : AI空蛙被嘎到語無倫次 05/21 23:11

推 justptt978  : 師爺 你給翻譯翻譯 05/21 23:12

推 brightest   : 台灣有林永隆教授開的NEUCHIPS 台灣也是有在努力跟 05/21 23:30

→ brightest   : 上 05/21 23:30

推 BigCockman  : 中國AI輸慘了 這波證明了中國軟體還是屌輸美國 中 05/21 23:36

→ BigCockman  : 國軟體始終是強在黨的長城+14億人口市場 科研還是 05/21 23:36

→ BigCockman  : 要靠美帝 05/21 23:36

推 bj45566     : 努力不代表有效，台灣在 NeurIPS, CVPR 這些 AI 頂 05/21 23:42

→ bj45566     : 會上發表績效就是很差，沒有世界級的競爭力 05/21 23:42

推 bj45566     : 其實中國的網路通訊、大數據、AI 研發水準反而是與 05/21 23:49

→ bj45566     : 美國差距最近的；中國在 CS 領域研發水準和歐美相差 05/21 23:49

→ bj45566     : 最遠的是 TCS (理論電腦科學、OSDI (作業系統設計) 05/21 23:49

→ bj45566     : 、SE (軟體工程),... 這些 05/21 23:49

推 bj45566     : TCS 這塊以色列超級可怕的，以一國之力在 ACM STOC 05/21 23:53

→ bj45566     :  會議發表績效和經典教科書市場都能和美國相抗衡 -- 05/21 23:53

→ bj45566     :  猶太人果真是全世界最聰明的民族！ 05/21 23:53

推 good5755    : 商湯之類的公司主要是視覺算法吧 不是靠樣本數夠多 05/21 23:57

→ good5755    : 試錯成本低算出來的？這篇文章主要是聚焦硬體算力 05/21 23:58

→ good5755    : 對岸自研晶片目前看起來只有阿里雲成氣候吧 05/21 23:59

推 bj45566     : 所有的深度學習 AI 都非常依賴海量資料 Training 05/21 23:59

噓 steak5566   : 中或贏 05/21 23:59

推 dangurer    : 這篇當科普還可以吧…也許有些太一廂情願? 05/22 00:01

推 bj45566     : 商湯能在電腦視覺領域領先還是演算法夠強 -- 領軍 05/22 00:01

→ bj45566     : 的香港中文大學教授團隊 05/22 00:01

推 atlaswhz    : 中國天網系統AI很強啊!可惜民間不能用 05/22 00:03

→ aegis43210  : 巨量資料分析未來是ASIC的天下了 05/22 00:03

→ good5755    : 人臉辨識前幾年很紅 現在看起來也只有蘋果堅持用 05/22 00:03

→ good5755    : 歐美國家個資問題 你敢用還不告死你 05/22 00:04

推 bj45566     : 其實這篇和中國其他的技術類長文和台灣國科會計畫 05/22 00:09

→ bj45566     : 書的寫作方法根本是孿生兄弟 XDDD (國科會時代，現 05/22 00:09

→ bj45566     : 在我不知道是否有改變) 05/22 00:09

推 bj45566     : 圖像辨識真正的主戰場在 CSI (犯罪鑑識) 和軍武(如 05/22 00:12

→ bj45566     : 智慧導彈)喔；以後還會擴及到精密手術輔助工具 05/22 00:12

→ bj45566     : 最後是高檔的機器人產業 05/22 00:13

推 bj45566     : ASIC 現在只能做硬體輔助加速 05/22 00:16

推 dannyko     : 一堆人沒仔細看內文…裡面有一個論點是對的，GPU雖 05/22 00:30

→ dannyko     : 然現在在算AI很強，但本質上還是圖像計算硬體，靠C 05/22 00:30

→ dannyko     : uda轉譯(連最強的V100 H100本質也是)如果今天有成 05/22 00:30

→ dannyko     : 熟且專門的AI計算硬體(狗和蘋果還沒搞出來的)，同 05/22 00:30

→ dannyko     : 樣使用先進晶片的狀況下，效能肯定會噴上去 05/22 00:30

→ dannyko     : 就看未來是老黃刀切得快 還是其他公司追得快 05/22 00:32

推 s860134     : 就問你自製硬體 軟體還是要相容 CUDA ? 05/22 00:45

→ s860134     : 還是你要自己重新土炮軟體架構? 05/22 00:45

→ s860134     : 自製很美好 等到真的要取代才骨感 05/22 00:45

推 bj45566     : 樓上，因為一堆開罵的人根本看不懂內文啊 -- 雖然這 05/22 00:51

→ bj45566     : 篇寫的也不算好而且有技術錯誤 05/22 00:51

→ bj45566     : 樓上是指 dannyko 網友 05/22 00:52

推 hanhsiangmax: 歷史科普優文推 05/22 00:53

推 bj45566     : 不過大部分人不知道 von Neumann Architecture vs. 05/22 00:56

→ bj45566     :  Harvard Architecture 也是正常啦，作者太賣弄術語 05/22 00:56

→ bj45566     : 了 05/22 00:56

推 bj45566     : 而且 GPU 架構是剛好蠻適合建構現在的深度學習類神 05/22 01:01

→ bj45566     : 經網路，但這個架構跟人類大腦真正的神經網路架構是 05/22 01:01

→ bj45566     : 相差非常非常大的 05/22 01:01

推 by19        : 真的廢言一堆 看不到重點 05/22 01:06

推 bj45566     : 因為這篇本質上是歷史科普文... 05/22 01:08

推 good5755    : 不就GPU和NPU的差別嗎 N家沒有搞NPU嗎? 05/22 01:09

噓 musie       : 廢文一篇 連Anthropic 都打錯 05/22 01:22

噓 goodjop     : 對面的文章 真的不用浪費時間 05/22 01:38

噓 bcza245682  : 廢話一堆 05/22 01:56

推 bj45566     : TPU, NPU, DPU,... 都是對 AI 做最佳化的嘗試而已， 05/22 02:03

→ bj45566     : NPU 也沒特別優秀 05/22 02:03

噓 a000000000  : 老黃自己塞惹tensor core  實際上4gpu+asic惹 05/22 02:14

→ a000000000  : 自研仔我看差不多可以收攤惹   整體戰力差太遠 05/22 02:15

→ a000000000  : 而估狗tpu主要還是針對DLRM  現在想改就三五年後惹 05/22 02:16

→ a000000000  : 搞不好LLM啥的又變形惹 05/22 02:17

→ rewisyoung  : 廢文看了打哈欠 車用這塊不分品牌老黃壟斷 會不知 05/22 02:20

→ rewisyoung  : 道的就外行人 05/22 02:20

推 asubelieve  :  認真說這篇文章說了太多假設性的東西而且沒有數據 05/22 02:26

→ asubelieve  : 佐證，從頭到尾都是自己的評論 05/22 02:26

噓 a000000000  : 車用ai市場"目前"沒很大   mpu霸王是Q康 05/22 02:26

推 dangurer    : 原來anthropic打錯XD 以為是公司名有藏梗 真像學生 05/22 02:36

→ dangurer    : 報告 在認真的外表下一堆資訊認知落差但認真考古 05/22 02:36

推 joey00      : 覺得歷史科普不錯啊 只是沒結論 05/22 03:22

噓 ksjr        : 這心得...原po484也沒把整篇給讀完蛤 05/22 03:33

噓 NEX4036     : 太長 05/22 04:01

噓 wahaha99    : 建議把這篇餵給ChatGPT4幫你做摘要 05/22 04:47

→ wahaha99    : 不然誰看得完 05/22 04:47

噓 kyova       : 文章太長，你以為股民看得下去嗎 05/22 04:49

噓 deepdish    : 太長END 05/22 05:02

推 shields5566 : 太長了 以為是在做大學生報告逆 05/22 05:23

推 dosiris     : 以後電腦 要不要再設計插一個TPU運算卡 05/22 05:34

→ SILee       : 其他家不是沒能力客製化出算力更強的。主要問題是卡 05/22 05:39

→ SILee       : 在硬體的開發時程很長很長，是用年為單位在算的。 05/22 05:39

→ SILee       : 根本跟不上現在AI模型以週為單位在變化演進的速度。 05/22 05:40

推 dosiris     : Google怎麼不賣PCIe插槽的TPU運算卡 我會想買說 05/22 05:40

→ SILee       : 等你開發出來，已經過時了。所以到頭來大家還是傾向 05/22 05:41

→ SILee       : 用nvidia的solution，比較有彈性。 05/22 05:41

推 dosiris     : 電腦端能插TPU卡/NPU卡的競爭時代一到 GG就訂單滿了 05/22 05:43

推 Justisaac   : 這東西就跟晶圓一樣，賺到錢的人自然會成為領先者 05/22 05:47

→ Justisaac   : 應該沒有人會認為Nvda沒有在研發別種運算核心吧.... 05/22 05:47

噓 hsiaoeddie  : 廢話一堆 05/22 07:15

推 gracefeather: 硬體發展當中，最慢的瓶頸是記憶體吧 05/22 07:28

噓 hschian     : 我就問TPU 製程節點還停留在什麼製成？ 05/22 07:58

→ hschian     : 然後NV用什麼製成？ 05/22 07:58

推 hansioux    : NVIDIA，台語翻成 Ing-uí-ta 英偉達，很像啊 05/22 08:05

噓 kevin31a2   : 名字都不敢打算三小研究 05/22 08:16

推 coolmark01  : 有商機才值得投入自研，不然都是浮雲不如繼續用NV既 05/22 08:35

→ coolmark01  : 有的商品 05/22 08:35

→ TrumpCard   : 好奇如果讓chart GPT 回應這篇”重點節錄”會顯示 05/22 08:44

→ TrumpCard   : 什麼？ 05/22 08:44

→ TAKEZOU     : 寫著麼多還是要跪著買NV 05/22 08:49

→ guowei616   : 公啥小 05/22 09:01

噓 superlive   : 什麼廢文浪費我時間 05/22 09:31

推 doomsday0728: 廢文 05/22 09:37

噓 x001611     : 中國廢話 05/22 10:04

→ videoproblem: 中或贏 05/22 10:19

噓 GooglePixel : 標題釣魚內文小說 結論就是科技巨頭不可能一直被NV 05/22 10:22

→ GooglePixel : DA收稅 能夠自己做ASIC可以省下很大成本 從GPU架構 05/22 10:22

→ GooglePixel : 上來看還是包了很多AI運算以外的東西 這些在特定應 05/22 10:22

→ GooglePixel : 用都是冗餘的可以節省的成本 05/22 10:22

噓 Benjamin901 : 純噓英偉達 05/22 10:22

推 xj654m3     : 阿里的困難最多XDD 05/22 10:53

推 hanmas      : 歷史科普不錯 05/22 10:59

→ k85564      : 寫那麼大一篇已知 05/22 11:04

→ k85564      : 然後沒有提到mi300/350 差評 05/22 11:06

推 to1322      : 整篇看完... 只有標題比較殺 05/22 11:39

推 king12272   : 這篇大概就是AI寫的 落落長但是沒有內容 05/22 12:56

推 wardraw     : 推科普 只要老黃價格控制得當 還是比自研成本來得低 05/22 14:45

噓 jamesho8743 : google tpu 還不是打不過老黄 GPU雖然不是針對AI設 05/22 18:58

→ jamesho8743 : 計 但是它有規模優勢 成本優勢 整體的性價比 能效 05/22 18:58

→ jamesho8743 : 比 沒有輸專用的chip 更別說軟體優勢了 05/22 18:58

噓 puffycat    : 廢文一篇！不能跑Cuda的TPU沒人要啦！即使TPU性能 05/24 14:13

→ puffycat    : 再好再省電再便宜，一句可以跑Cuda就謝謝再聯絡！ 05/24 14:13
