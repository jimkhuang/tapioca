作者waitrop (嘴砲無雙)看板Stock標題Re: [新聞] AI晶片競爭開跑，谷歌公布第四代TPU，宣時間Fri Apr  7 15:09:01 2023
※ 引述《ynlin1996 (.)》之銘言：
: 原文標題：
: AI晶片競爭開跑，谷歌公布第四代TPU，宣稱比輝達A100晶片更快更節能
: 原文連結：
: https://bit.ly/3meYAG8
: 發布時間：
: 2023.4.6
: 記者署名：
: 茋郁
: 原文內容：
: 谷歌於2023年4月5日公佈了其用於訓練AI模型的超級電腦的新細節，稱該系統比輝達的同類型系統A100更快、更節能。雖然現今大多數涉足AI領域的公司的處理能力都來自輝達的晶片，但谷歌設計自己客製化晶片Tensor Processing Unit（TPU）期望能夠推動其在AI研究。
: 谷歌是在COVID-19大流行高峰期間宣布了其Tensor晶片，當時從電子到汽車的企業面臨晶片短缺的困境。由於谷歌看到蘋果在客製化晶片的投入，幫助了iPhone以及其他產品的開發。因此最初該晶片是為其Pixel智慧型手機提供動力，可是歷經幾年之後，谷歌將TPU延伸至更多領域，可見得其開發方面取得了長足的進步。
: 如今谷歌90%以上的AI訓練工作都使用這些晶片，即通過模型提供數據的過程，使它們在諸如類似人類的文字查詢或生成影像等任務中發揮作用。
: 谷歌TPU現在已經是第四代了。之所以世代晶片發展那麼迅速的關鍵在於，谷歌使用AI來設計其TPU晶片。谷歌聲稱，與人類花費數月時間設計晶片相比，使用AI設計晶片僅需要6小時即可完成設計過程。
: 一旦採用AI設計晶片，產品迭代都在迅速發生，這就是TPU進入第四代的原因。未來隨著生成式AI的快速發展，將造成大型語言模型的規模呈現爆炸式成長，這意味著它們太大而無法儲存在單顆晶片上。所以谷歌客製化開發的光訊號交換器將4,000多顆晶片串在一起成為一台超級電腦，以幫助連接各個機器。此外，微軟也是將晶片拼接在一起以滿足OpenAI的研究需求。
: 谷歌指出，PaLM模型——其迄今為止最大的公開披露的語言模型——通過在50天內將其拆分到4,000顆晶片超級電腦中的兩個來進行訓練。更重要的是，其超級電腦可以輕鬆地動態重新配置晶片之間的連接，有助於避免出現問題並進行調整以提高性能。
: 谷歌除了自己使用TPU之外，一家新創公司Midjourney使用該系統訓練其模型，該模型在輸入幾句文字後即可生成新影像。
: 可是谷歌並沒有將其第四代晶片與輝達目前的旗艦H100晶片進行比較。谷歌暗示它可能正在開發一種新的TPU，它將與輝達H100展開競爭。
: 總之，隨著生成式AI的崛起，AI晶片的開發腳步將成為下一個廠商積極介入的領域，以搶奪未來商機。
: 心得/評論：
: Google公布第四代TPU，並宣稱比NVIDIA A100更快更節能，其客戶Midjourney利用TPU訓練其模型再輸入文字後生成圖片。

周末下班閒聊,
發現tech版沒有這篇文章,
那就在這裡討論好了,
其實我說的東西都是網路 google+wiki 就有的東西,

先簡單說明一下現代化的ML DeepLearning的基本概念,
其實就是用暴力法把整個model 放進去 GPU/TPU 裏頭training/inference,
這會有兩個最大的限制,
硬體速度跟GPU/TPU記憶體容量,
硬體太慢跑不動或跑太久,
GPU/TPU記憶體容量太小的話是連跑都不能跑,
我幾年前在板上分享的TQQQ DeepLearning 預測股價的模型,
需要8GB 的 GPU VRAM

維基小百科之後就回到主題: ChatGPT,
目前所有的聊天機器人模型通通都是LLM,
https://en.wikipedia.org/wiki/Wikipedia:Large_language_models
GPT-3 需要VRAM 175GB,
股歌微軟OpenAI 這幾家的模型一定遠遠超過這個大小,
大上數百倍都有可能,
我是做硬體晶片的,
不要問我model 這塊的問題,
所以現在這些聊天機器人模型最大的問題是整個系統無法塞下model training,
所以這篇文章的重點在這句話:
"谷歌客製化開發的光訊號交換器將4,000多顆晶片串在一起",
只能這麼做才有辦法塞下這麼大的model training,
Nvidia 也有類似的技術叫做 NVLink,
可以把電腦上數個Nvidia 顯卡串聯運算,
但是如果模型大到一台電腦顯卡全部插滿都塞不下運算的話,
我就不知道Nvidia 如何解決,
但是我相信Nvidia 一定有解決的方法

所以現在問題已經提升到整個系統端,
而不是我的晶片跑多快這麼簡單的問題,
軟體跟模型要如何切割如何分配塞入多台超級電腦,
硬體方面如何塞入最大的模型做運算

說個題外話,
我幾年前做了一個project, AI 晶片與系統研發的project,
最近拿到公司的大獎,
不是新聞這個TPU,
我跟開發model 的對口同事討論過,
是否能夠拿我們開發的這個系統來跑Chat model,
得到答案是否定的,
至少現階段還無法做到,
原因很簡單, VRAM 不夠大, 塞不下model,
但是他們model 部門也有在想辦法要細切 model 看可不可以塞進去這個系統,
所以速度已經不是最重要的考量了,
至於速度,
兩年前應該是全世界最快的晶片+系統,
可惜 VRAM 不夠大,
原本運用方向就不是要做chat

推 c928        : 太專業了 04/07 15:14

推 summer08818 : 一台電腦塞不下換一個機房塞不塞得下 我現在在弄XD 04/07 15:17

推 jagger      : 推 04/07 15:24

推 roseritter  : 分別塞 細切的方案感覺成本比較省 04/07 15:29

→ acininder   : 內行的就知道 大模型最重要的其實是VRAM而非速度 04/07 15:30

→ roseritter  : 想起古早56K年代 用PC幫忙解外星人的project 04/07 15:30

→ acininder   : transformer系的瓶頸都是記憶體牆 04/07 15:30

推 LieTo       : 用NVSwitch or IB(InfiniBand) 04/07 15:30

→ acininder   : 包含記憶體容量和存取速度 運算速度反而是其次 04/07 15:31

推 wahaha711233: 類似腦細胞神經連結 30年前的超大電腦又回來了 04/07 15:33

→ acininder   : 不過微軟提供給openai的解決方案不是NVlink 04/07 15:36

→ acininder   : 而是用InfiniBand把幾萬張A100串起來 04/07 15:36

推 speculator  : 比較好奇文中說的AI設計晶片是不是真的？ 04/07 15:55

推 aegis43210  : AI設計晶片應該指的是軟體定義晶片，也就是Coarse G 04/07 16:11

→ aegis43210  : rained Reconfigurable Arra，目前這塊最成功的是思 04/07 16:11

→ aegis43210  : 科，其網路交換器及虛擬化處理器都有用到類似架構 04/07 16:11

推 donkilu     : 分散運算要面對資料傳輸的瓶頸 也有它本身的限制 04/07 16:11

推 wrt         : NV之前買下mellanox 04/07 16:12

→ wrt         : 可以把多台server用網卡串起來 04/07 16:12

推 donkilu     : 短期內搞研究的還是要用類超級電腦 分散運算我覺得 04/07 16:15

→ donkilu     : 是用在一些比較成熟的狀況 例如iphone pixel的model 04/07 16:16

推 aegis43210  : Graphcore的IPU也是走單一超級電腦路線，x86陣營則 04/07 16:17

→ aegis43210  : 是嘗試用新一代AMX指令集來搶食訓練大餅，目前是期 04/07 16:17

→ aegis43210  : 待我國新創公司能在AI推理有所成果 04/07 16:17

推 donkilu     : 畢竟training目前看起來還是需要大力出奇蹟... 04/07 16:20

推 DB2         : 就說了是比系統不是比晶片，原文推文一堆外行 04/07 16:22

→ acake       : 感謝分享 04/07 16:24

推 gs13010     : 其實NV也已經佈局switch了，mellanox 就是做IB連結 04/07 16:29

→ gs13010     : 的公司 04/07 16:29

推 g0428168    : NV的DGX其實應該就有用Mallanox Switch去串全部機台 04/07 16:52

推 Chilloutt   : 大頻寬的光交換器準備要商轉了，不怕啦 04/07 17:48

推 roserule    : 問個笨問題 TSLA 不是有個  DoJo超級電腦？ 是不是 04/07 18:48

→ roserule    : 跟文中所提的超級電腦同一個類別？ 04/07 18:48

推 soar625     : 很專業，所以可以買啥標的 04/07 19:22

→ sleeplist   : 請問版上鄉民，像現在高通的晶片，也可以直接作出 04/07 19:26

→ sleeplist   : GPU 使用目前最重要的AI類功能嗎，還是說仍然必須 04/07 19:26

→ sleeplist   : 使用到 Nvidia的技術？ 04/07 19:27

→ sleeplist   : 也就是是否就算用高通晶片作出 GPU，也會因為因為 04/07 19:27

→ sleeplist   : Nvidia的某些架構或者特殊技術，所以沒辦法用？ 04/07 19:27

→ sleeplist   : 而且高通這晶片其實去年4月就現市了 04/07 19:38

→ sleeplist   : 我實在不太清楚目前這些是都能直接用還是有架構問題 04/07 19:39

推 nkfish      : 推個解釋的連文組也能懂 04/07 22:27

推 cmshow      : 推好人分享 04/08 11:14

推 cloudofsky  : 推 04/08 13:33

推 stlinman    : 推，分享! 04/08 15:50

→ stlinman    : 算力的價值建立在產出吧，覺得可以從AI產出去找股票 04/08 15:50

推 noddle      : 推分享 04/08 18:21

推 TimoBall    : 高通 edge 端 inference 可以吧 你的手機不知道跑多 08/23 19:25

→ TimoBall    : 少 ai model 08/23 19:25

→ TimoBall    : nv 除了硬體, cuda 也從另一種層面讓 application 08/23 19:27

→ TimoBall    : 綁他 08/23 19:27

→ TimoBall    : amd 還要抄他們 lib 苦苦追趕 08/23 19:27
